{"cells":[{"cell_type":"markdown","metadata":{"_id":"D89B0EAB542642F98900CCC4D4E49360","jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"E0B6AB7C9D0D4DE9B7662A3CA22DBC78","runtime":{"status":"default","execution_status":null,"is_visible":false},"notebookId":"684bc75fbe751fb8601003a2"},"source":"# 实践作业  \n作业说明：已给出关键代码提示，可根据自身能力选择基础或进阶作业完成，想多尝试也可以两部分都完成。  \n "},{"cell_type":"markdown","metadata":{"_id":"3973CB552C654162B4E8BB8A56DD2E46","jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"C46637F7EF6142E781B888B8D04F80D7","runtime":{"status":"default","execution_status":null,"is_visible":false},"notebookId":"684bc75fbe751fb8601003a2"},"source":"### 基础作业  \n 根据教案中的学习代码，尝试做以下的调整，需要有尝试的过程代码和结果的文字总结。  \n"},{"cell_type":"markdown","metadata":{"_id":"B923221CD9914A519921F0932C865E21","jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"7221FB38C4E4470F97251A2898944D45","runtime":{"status":"default","execution_status":null,"is_visible":false},"notebookId":"684bc75fbe751fb8601003a2"},"source":"**1. 尝试不同的LoRA配置参数，对比性能差异**  \n**概述**：在实际开发中，不同的LoRA配置参数会影响模型的性能，尝试不同的参数配置确定模型的最佳性能。  \n**方法**：选则固定的评估指标，然后排列各种参数配置，观察不同参数配置下的评估指标的变化，当某个参数配置的评估指标最优时，则认为该参数配置最优  \n- rank (r): 低秩矩阵的维度  \n典型范围: 2-256，值越大表示适配能力越强，但也可能过拟合  \n- alpha (α): 缩放因子  \n控制适配强度，通常设置为rank的倍数(如2×rank)  \n- dropout: 防止过拟合  \n典型值: 0-0.2  \n- target_modules: 应用LoRA的模块  \n常见选择: query/key/value层或全连接层"},{"cell_type":"markdown","metadata":{"_id":"035BA4B679484E9F9520BE442BE3EEFE","jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"558A4BA2E5954FD987F3C138364E97F1","runtime":{"status":"default","execution_status":null,"is_visible":false},"notebookId":"684bc75fbe751fb8601003a2"},"source":"**2. 设计并实现自定义的评估指标**  \n- **概述**：在LLM中，还有许多不同评估角度的评估指标，例如BULE，Rouge，N-Gram，可手动尝试不同的指标对模型性能的影响  \n- **方法**：例如使用BULE，Rouge作为评估指标，然后进行训练，训练后进行实际测评（文本生成、翻译等任务），观察微调后的模型性能是否有改善，若有，则认为这些评估指标有效"},{"cell_type":"code","metadata":{"_id":"791E3B9E0657457980E71F1CF064D32D","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"10DB068C53FC419CB3DE14EC175C16FC","notebookId":"684bc75fbe751fb8601003a2","trusted":true},"source":"#你的代码过程\n\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nfrom rouge_chinese import Rouge\nimport jieba\nimport numpy as np\n\nclass TextEvaluator:\n    def __init__(self):\n        \"\"\"初始化评估器\"\"\"\n        self.rouge = Rouge()\n        self.smooth = SmoothingFunction()\n    \n    def calculate_bleu(self, reference, candidate):\n        \"\"\"\n        计算BLEU分数\n        \n        参数:\n            reference (str): 参考文本\n            candidate (str): 候选文本\n            \n        返回:\n            float: BLEU分数\n        \"\"\"\n        # 将文本分词\n        reference_tokens = list(jieba.cut(reference))\n        candidate_tokens = list(jieba.cut(candidate))\n        \n        # 计算BLEU分数\n        try:\n            score = sentence_bleu([reference_tokens], candidate_tokens, \n                                smoothing_function=self.smooth.method1)\n            return score\n        except Exception as e:\n            print(f\"计算BLEU分数时出错: {e}\")\n            return 0.0\n    \n    def calculate_rouge(self, reference, candidate):\n        \"\"\"\n        计算ROUGE分数\n        \n        参数:\n            reference (str): 参考文本\n            candidate (str): 候选文本\n            \n        返回:\n            dict: 包含ROUGE-1、ROUGE-2和ROUGE-L分数的字典\n        \"\"\"\n        try:\n            scores = self.rouge.get_scores(candidate, reference)[0]\n            return {\n                'rouge-1': scores['rouge-1']['f'],\n                'rouge-2': scores['rouge-2']['f'],\n                'rouge-l': scores['rouge-l']['f']\n            }\n        except Exception as e:\n            print(f\"计算ROUGE分数时出错: {e}\")\n            return {'rouge-1': 0.0, 'rouge-2': 0.0, 'rouge-l': 0.0}\n    \n    def evaluate_text(self, reference, candidate):\n        \"\"\"\n        综合评估文本质量\n        \n        参数:\n            reference (str): 参考文本\n            candidate (str): 候选文本\n            \n        返回:\n            dict: 包含所有评估指标的字典\n        \"\"\"\n        # 计算BLEU分数\n        bleu_score = self.calculate_bleu(reference, candidate)\n        \n        # 计算ROUGE分数\n        rouge_scores = self.calculate_rouge(reference, candidate)\n        \n        # 返回所有评估指标\n        return {\n            'bleu': bleu_score,\n            **rouge_scores\n        }\n\n# 测试代码\nif __name__ == \"__main__\":\n    # 创建评估器实例\n    evaluator = TextEvaluator()\n    \n    # 测试文本\n    reference = \"这是一个测试文本，用于评估文本生成质量。\"\n    candidate = \"这是一个测试文本，用来评估生成文本的质量。\"\n    \n    # 计算评估指标\n    scores = evaluator.evaluate_text(reference, candidate)\n    \n    # 打印结果\n    print(\"\\n评估结果:\")\n    print(f\"BLEU分数: {scores['bleu']:.4f}\")\n    print(f\"ROUGE-1分数: {scores['rouge-1']:.4f}\")\n    print(f\"ROUGE-2分数: {scores['rouge-2']:.4f}\")\n    print(f\"ROUGE-L分数: {scores['rouge-l']:.4f}\")","outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"_id":"80E196308D9540C7A2773A74252908F9","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"DC6ABBDFCB164AB19BBB725C87079B23","notebookId":"684bc75fbe751fb8601003a2","trusted":true},"source":"","outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"_id":"D5D34B530771492B9B3119DAA8354C32","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"64FB39AB01894D318F7D846E516AC107","notebookId":"684bc75fbe751fb8601003a2","trusted":true},"source":"","outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"_id":"56B9912E919F4DAAABE43EE20CC85848","jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"97E271E31C6749E1954909AB9EF6D435","runtime":{"status":"default","execution_status":null,"is_visible":false},"notebookId":"684bc75fbe751fb8601003a2"},"source":"### 进阶作业  \n**将微调方法应用到其他语言**  \n- **概述**：微调特点成本低、效用高，基座大模型可以通过微调满足特定领域的任务，微调成为是业界最常用的方法，可使用挂载好的韩语数据集，学习教案中的阿拉伯语微调教程，完成大模型学习韩语的任务  \n- **方法**：在完成基础作业的基础上（确定了Lora配置参数与评估指标）， 给大模型喂入韩语，完成微调"},{"cell_type":"code","metadata":{"_id":"7B469632F5D24434833AF23E47A1F3B0","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"AB5909ADD04A4C938F94235C632B1088","notebookId":"684bc75fbe751fb8601003a2","trusted":true},"source":"from typing import List, Dict, Set\n\ndef tokenize_text(text: str, lang: str = None) -> List[str]:\n    \"\"\"根据不同语言使用相应的分词器\n    \n    Args:\n        text: 要分词的文本\n        lang: 语言代码（如果为None则自动检测）\n    \n    Returns:\n        分词后的词语列表\n    \"\"\"\n    if not text:\n        return []\n        \n    # 如果没有指定语言，进行语言检测\n    if lang is None:\n        lang = detect_language(text)\n    \n    # 根据语言选择分词方法\n    if lang == 'zh':\n        # 中文使用jieba分词\n        return [word for word in jieba.cut(text) if word.strip()]\n    elif lang == 'th':\n        # 泰语使用pythainlp分词\n        return [word for word in thai_tokenize(text) if word.strip()]\n    elif lang == 'ko':\n        # 韩语分词\n        if mecab:\n            try:\n                # 使用mecab进行分词，过滤掉助词等\n                return [word for word, pos in mecab.pos(text) \n                       if word.strip() and not pos.startswith('J')]\n            except Exception:\n                pass\n        \n        # 如果mecab不可用或失败，使用基本分词\n        words = []\n        current_word = ''\n        for char in text:\n            if '\\uAC00' <= char <= '\\uD7AF' or char.isalnum():\n                current_word += char\n            else:\n                if current_word:\n                    words.append(current_word)\n                current_word = ''\n                if not char.isspace():\n                    words.append(char)\n        if current_word:\n            words.append(current_word)\n        return [w for w in words if w.strip()]\n    \n    elif lang == 'ru':\n        # 俄语分词 - 使用基本的razdel分词\n        return [token.text for token in ru_tokenize(text) \n                if token.text.strip() and not all(c in '.,!?;:()[]{}' for c in token.text)]\n    \n    if lang == 'ar':\n        # 标准化阿拉伯文本中的hamza字符(ء/أ/إ/ؤ/ئ)为统一形式，原因是阿拉伯语的变音符号会影响分词结果\n        text = araby.normalize_hamza(text)\n        # 标准化alef字符(آ/إ/أ/ا)为基本的alef(ا)，原因是阿拉伯语的alef字符会影响分词结果\n        text = araby.normalize_alef(text)\n        # 使用araby库对阿拉伯文本进行分词\n        words = araby.tokenize(text)\n        # 过滤分词结果:\n        # 1. 使用strip()去除首尾空白字符\n        # 2. 排除只包含标点符号的词\n        # 3. 返回过滤后的词列表\n        return [word for word in words if word.strip() and not all(c in '.,!?;:()[]{}' for c in word)]\n    else:\n        # 其他语言使用空格分词\n        return [word for word in text.split() if word.strip()]\n\ndef extract_domain_terms(domain_texts: List[str], general_texts: List[str] = None, top_n: int = 2000) -> List[str]:\n    \"\"\"使用改进的多语言分词方法提取领域术语\"\"\"\n    print(\"开始提取领域术语...\")\n    \n    # 按语言分组处理文本\n    lang_texts = defaultdict(list)\n    for text in domain_texts:\n        lang = detect_language(text)\n        if lang != 'unknown':\n            lang_texts[lang].append(text)\n    \n    print(\"\\n文本语言分布:\")\n    for lang, texts in lang_texts.items():\n        print(f\"- {lang}: {len(texts)} 条\")\n    \n    # 分语言处理并提取术语\n    all_terms = []\n    for lang, texts in lang_texts.items():\n        print(f\"\\n处理{lang}语言文本...\")\n        \n        # 使用语言专属工具提取术语\n        lang_terms = set()\n        for text in tqdm(texts, desc=f\"处理{lang}语言文本\"):\n            terms = extract_domain_terms_by_language(text, lang)\n            lang_terms.update(terms)\n        \n        # 将术语集合转换为列表,方便后续排序操作\n        lang_terms = list(lang_terms)\n\n        # 计算每个术语在文本中出现的频率\n        # 1. 遍历每个文本\n        # 2. 对每个文本重新提取术语\n        # 3. 使用Counter统计所有术语的频率\n        term_freq = Counter(t for text in texts for t in extract_domain_terms_by_language(text, lang))\n\n        # 对术语列表进行排序:\n        # 1. 首要排序依据是术语出现频率(term_freq[x])\n        # 2. 次要排序依据是术语长度(len(x))\n        # reverse=True表示按降序排列,即频率高的和长度长的排在前面\n        lang_terms.sort(key=lambda x: (term_freq[x], len(x)), reverse=True)\n        \n        # 根据语言数量平均分配术语数量配额\n        # 1. top_n是总的期望术语数量\n        # 2. len(lang_texts)是语言种类数\n        # 3. 对每种语言,只取配额内的高频长术语\n        # //是整除运算符,用于计算每种语言分配的术语数量配额\n        # 例如:如果top_n=2000,有4种语言,则每种语言分配2000//4=500个术语\n        # 这里的:是切片操作符,表示从列表开头取到指定位置\n        # //是整除运算符,例如10//3=3\n        # 所以lang_terms[:top_n // len(lang_texts)]表示:\n        # 1. 先计算top_n除以语言数量的整除结果n\n        # 2. 然后从lang_terms列表中取前n个元素\n        selected_terms = lang_terms[:top_n // len(lang_texts)]\n        all_terms.extend(selected_terms)\n        \n        print(f\"{lang}语言提取了 {len(selected_terms)} 个术语\")\n        if selected_terms:\n            print(f\"{lang}语言术语示例:\")\n            for term in selected_terms[:5]:\n                print(f\"  - {term}\")\n    \n    return all_terms\n\n\n\ndef extract_domain_terms_by_language(text: str, lang: str) -> List[str]:\n    \"\"\"使用语言专属工具提取领域术语\"\"\"\n    terms = []\n    \n    if lang == 'ko':\n        try:\n            # 使用KoNLPy的Mecab分析器\n            if mecab:\n                # 提取名词和专有名词\n                pos_tags = mecab.pos(text)\n                terms = [word for word, pos in pos_tags \n                        if pos.startswith('NN') or pos.startswith('NNP')]\n        except Exception as e:\n            print(f\"韩语处理失败: {e}\")\n    \n    elif lang == 'ru':\n        try:\n            # 使用pymorphy2进行形态分析\n            if morph:\n                words = [t.text for t in ru_tokenize(text)]\n                for word in words:\n                    parsed = morph.parse(word)[0]\n                    # 提取名词和专有名词\n                    if 'NOUN' in parsed.tag or 'Name' in parsed.tag:\n                        terms.append(word)\n        except Exception as e:\n            print(f\"俄语处理失败: {e}\")\n    \n    elif lang == 'th':\n        try:\n            # 使用pythainlp进行词性标注\n            words = thai_tokenize(text)\n            pos_tags = pythainlp.tag.pos_tag(words)\n            # 提取名词和专有名词\n            terms = [word for word, pos in pos_tags \n                    if pos.startswith('N') or pos.startswith('PROPN')]\n        except Exception as e:\n            print(f\"泰语处理失败: {e}\")\n    \n    elif lang == 'ar':\n        try:\n            # 使用pyarabic进行基本处理\n            text = araby.strip_tashkeel(text)  # 移除变音符号\n            words = araby.tokenize(text)\n            # 提取可能的术语（长度大于3的词）\n            terms = [w for w in words if len(w) > 3 and not any(c.isdigit() for c in w)]\n        except Exception as e:\n            print(f\"阿拉伯语处理失败: {e}\")\n    \n    return terms\n\n","outputs":[],"execution_count":2},{"cell_type":"code","metadata":{"id":"98831B8DAB53423E8C8B1A843EF533BE","notebookId":"684bc75fbe751fb8601003a2","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"","outputs":[],"execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python","nbconvert_exporter":"python","file_extension":".py","version":"3.5.2","pygments_lexer":"ipython3"}},"nbformat":4,"nbformat_minor":0}