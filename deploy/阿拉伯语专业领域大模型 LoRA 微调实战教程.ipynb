{"cells":[{"cell_type":"markdown","metadata":{"trusted":true,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"D89B0EAB542642F98900CCC4D4E49360","runtime":{"status":"default","execution_status":null,"is_visible":false},"scrolled":false,"notebookId":"67f4d858eb20abf0847c425e"},"source":"# 阿拉伯语专业领域大模型 LoRA 微调实战教程"},{"cell_type":"markdown","metadata":{"id":"93AFBCA685484C9395A5394B515D61F4","notebookId":"67f4d858eb20abf0847c425e","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"## 环境安装（包下载）  \n使用https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple 清华园镜像  \n"},{"cell_type":"code","metadata":{"id":"CFAA3AE95B4E497994BCFF2D7F1A624A","notebookId":"67f4d858eb20abf0847c425e","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"!ls","outputs":[{"output_type":"stream","name":"stdout","text":"merge_and_save_20250317_075302.log  training_20250428_070925.log\r\nmerge_and_save_20250317_075318.log  training_20250428_071316.log\r\ntraining_20250316_155425.log\t    training_20250428_091335.log\r\ntraining_20250316_155535.log\t    training_20250428_091549.log\r\ntraining_20250428_064926.log\t    training_20250428_091657.log\r\ntraining_20250428_065344.log\t    training_20250428_091742.log\r\ntraining_20250428_065821.log\t    training_20250428_092025.log\r\ntraining_20250428_070520.log\t    training_20250429_075124.log\r\ntraining_20250428_070643.log\t    training_20250507_091912.log\r\ntraining_20250428_070805.log\t    training_20250507_092655.log\r\n"}],"execution_count":1},{"cell_type":"code","metadata":{"id":"810CF482643D4985AFA558B9F60AF78D","notebookId":"67f4d858eb20abf0847c425e","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":true,"scrolled":false},"source":"!pip install peft -i https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple","outputs":[{"output_type":"stream","name":"stdout","text":"\u001b[33mDEPRECATION: Loading egg at /opt/conda/lib/python3.11/site-packages/papermill-2.3.1-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n\u001b[0mLooking in indexes: https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple\nCollecting peft\n  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/68/85/8e6ea3d1089f2b6de3c1cd34bbbd7560912af9d34b057be3b8b8fefe1da3/peft-0.15.2-py3-none-any.whl (411 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.1/411.1 kB\u001b[0m \u001b[31m724.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from peft) (1.25.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from peft) (24.0)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from peft) (5.9.8)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from peft) (6.0.1)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.11/site-packages (from peft) (2.5.1)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (from peft) (4.48.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from peft) (4.66.4)\nCollecting accelerate>=0.21.0 (from peft)\n  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/63/b1/8198e3cdd11a426b1df2912e3381018c4a4a55368f6d0857ba3ca418ef93/accelerate-1.6.0-py3-none-any.whl (354 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.7/354.7 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: safetensors in /opt/conda/lib/python3.11/site-packages (from peft) (0.4.3)\nRequirement already satisfied: huggingface_hub>=0.25.0 in /opt/conda/lib/python3.11/site-packages (from peft) (0.28.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.25.0->peft) (3.17.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.25.0->peft) (2024.5.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.25.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.25.0->peft) (4.12.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.1.3)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers->peft) (2024.5.15)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.11/site-packages (from transformers->peft) (0.21.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.2.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2024.2.2)\nInstalling collected packages: accelerate, peft\nSuccessfully installed accelerate-1.6.0 peft-0.15.2\n"}],"execution_count":2},{"cell_type":"code","metadata":{"trusted":true,"collapsed":true,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"4FC61B519BE24DD590C9913A15FBDA95","scrolled":false,"notebookId":"67f4d858eb20abf0847c425e"},"source":"!pip install scikit-learn -i https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple","outputs":[{"output_type":"stream","name":"stdout","text":"\u001b[33mDEPRECATION: Loading egg at /opt/conda/lib/python3.11/site-packages/papermill-2.3.1-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n\u001b[0mLooking in indexes: https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple\nCollecting scikit-learn\n  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/a8/f3/62fc9a5a659bb58a03cdd7e258956a5824bdc9b4bb3c5d932f55880be569/scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.19.5 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.25.0)\nCollecting scipy>=1.6.0 (from scikit-learn)\n  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/32/ea/564bacc26b676c06a00266a3f25fdfe91a9d9a2532ccea7ce6dd394541bc/scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn)\n  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/da/d3/13ee227a148af1c693654932b8b0b02ed64af5e1f7406d56b088b57574cd/joblib-1.5.0-py3-none-any.whl (307 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m-:--:--\u001b[0m\n\u001b[?25hCollecting threadpoolctl>=3.1.0 (from scikit-learn)\n  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/32/d5/f9a850d79b0851d1d4ef6456097579a9005b31fea68726a4ae5f2d82ddd9/threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\nInstalling collected packages: threadpoolctl, scipy, joblib, scikit-learn\nSuccessfully installed joblib-1.5.0 scikit-learn-1.6.1 scipy-1.15.2 threadpoolctl-3.6.0\n"}],"execution_count":3},{"cell_type":"code","metadata":{"trusted":true,"collapsed":true,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"16BEDEF83DF24B65ABB0E52615CECDCB","scrolled":false,"notebookId":"67f4d858eb20abf0847c425e"},"source":"!pip install sentence_transformers -i https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple","outputs":[{"output_type":"stream","name":"stdout","text":"\u001b[33mDEPRECATION: Loading egg at /opt/conda/lib/python3.11/site-packages/papermill-2.3.1-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n\u001b[0mLooking in indexes: https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple\nCollecting sentence_transformers\n  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/45/2d/1151b371f28caae565ad384fdc38198f1165571870217aedda230b9d7497/sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.7/345.7 kB\u001b[0m \u001b[31m746.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.11/site-packages (from sentence_transformers) (4.48.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from sentence_transformers) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from sentence_transformers) (2.5.1)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (from sentence_transformers) (1.6.1)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.11/site-packages (from sentence_transformers) (1.15.2)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.11/site-packages (from sentence_transformers) (0.28.1)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.11/site-packages (from sentence_transformers) (10.3.0)\nRequirement already satisfied: typing_extensions>=4.5.0 in /opt/conda/lib/python3.11/site-packages (from sentence_transformers) (4.12.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.17.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.5.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.3)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.25.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.5.15)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.3)\nRequirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->sentence_transformers) (1.5.0)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->sentence_transformers) (3.6.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.2.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.2.2)\nInstalling collected packages: sentence_transformers\nSuccessfully installed sentence_transformers-4.1.0\n"}],"execution_count":4},{"cell_type":"code","metadata":{"id":"E36A141DA8B24B018A9ED2B6E067BD52","notebookId":"67f4d858eb20abf0847c425e","jupyter":{},"collapsed":true,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"!pip install torch -i https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple","outputs":[{"output_type":"stream","name":"stdout","text":"\u001b[33mDEPRECATION: Loading egg at /opt/conda/lib/python3.11/site-packages/papermill-2.3.1-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n\u001b[0mLooking in indexes: https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple\nRequirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.5.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.3)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch) (2024.5.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.11/site-packages (from torch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.11/site-packages (from torch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.11/site-packages (from torch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.11/site-packages (from torch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.11/site-packages (from torch) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.11/site-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.11/site-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n"}],"execution_count":5},{"cell_type":"code","metadata":{"id":"E67B439074DB4FC8895455C74F73168D","notebookId":"67f4d858eb20abf0847c425e","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"!pip install jieba -i https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple","outputs":[{"output_type":"stream","name":"stdout","text":"\u001b[33mDEPRECATION: Loading egg at /opt/conda/lib/python3.11/site-packages/papermill-2.3.1-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n\u001b[0mLooking in indexes: https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple\nCollecting jieba\n  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/c6/cb/18eeb235f833b726522d7ebed54f2278ce28ba9438e3135ab0278d9792a2/jieba-0.42.1.tar.gz (19.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: jieba\n  Building wheel for jieba (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314459 sha256=07ce8edc2b1d4a7c973e3a5b787b336bbeebcc21a2068e3adc665149381cbd93\n  Stored in directory: /home/mw/.cache/pip/wheels/6a/9e/05/44c9b4f0b5d56fe19a808b4a781888f4723011b6d1129309ef\nSuccessfully built jieba\nInstalling collected packages: jieba\nSuccessfully installed jieba-0.42.1\n"}],"execution_count":6},{"cell_type":"code","metadata":{"id":"5395F0B9CF5B4698948A0D31C433C9DC","notebookId":"67f4d858eb20abf0847c425e","jupyter":{},"collapsed":true,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"!pip install langdetect -i https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple","outputs":[{"output_type":"stream","name":"stdout","text":"\u001b[33mDEPRECATION: Loading egg at /opt/conda/lib/python3.11/site-packages/papermill-2.3.1-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n\u001b[0mLooking in indexes: https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple\nCollecting langdetect\n  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/0e/72/a3add0e4eec4eb9e2569554f7c70f4a3c27712f40e3284d483e88094cc0e/langdetect-1.0.9.tar.gz (981 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.11/site-packages (from langdetect) (1.16.0)\nBuilding wheels for collected packages: langdetect\n  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993227 sha256=59d3edc51c06fd0eb7c23aed3dfcdf7be82829121abd139d882328597f7a316f\n  Stored in directory: /home/mw/.cache/pip/wheels/e0/d5/c9/ef03f6107c39bd4f1aa7286f1e2e8573f6ff2824f981aece09\nSuccessfully built langdetect\nInstalling collected packages: langdetect\nSuccessfully installed langdetect-1.0.9\n"}],"execution_count":7},{"cell_type":"code","metadata":{"id":"A96BAA86A5BC4754BFCE0CA39E2F6DA7","notebookId":"67f4d858eb20abf0847c425e","jupyter":{},"collapsed":true,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"!pip install pythainlp -i https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple","outputs":[{"output_type":"stream","name":"stdout","text":"\u001b[33mDEPRECATION: Loading egg at /opt/conda/lib/python3.11/site-packages/papermill-2.3.1-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n\u001b[0mLooking in indexes: https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple\nCollecting pythainlp\n  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/3f/83/f8f1374e1f85a0767668c86d472e012c7bda491377b5852e0d71366d1dda/pythainlp-5.1.1-py3-none-any.whl (19.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.3/19.3 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: requests>=2.31 in /opt/conda/lib/python3.11/site-packages (from pythainlp) (2.32.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.31->pythainlp) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.31->pythainlp) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.31->pythainlp) (2.2.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.31->pythainlp) (2024.2.2)\nInstalling collected packages: pythainlp\nSuccessfully installed pythainlp-5.1.1\n"}],"execution_count":8},{"cell_type":"markdown","metadata":{"id":"7A34A4B9BDF54B218CA3B6492E2EA9ED","notebookId":"67f4d858eb20abf0847c425e","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"## 环境安装--hugging face国内镜像设置  \n这是为了更快地加载模型"},{"cell_type":"code","metadata":{"id":"D5FD19798B824EA495CC97F14B3D83CA","notebookId":"67f4d858eb20abf0847c425e","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"import os\nimport logging\n# 设置 HF_ENDPOINT 环境变量\nos.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\nlogging.basicConfig(level=logging.INFO)\nprint(os.environ.get('HF_ENDPOINT'))","outputs":[{"output_type":"stream","name":"stdout","text":"https://hf-mirror.com\n"}],"execution_count":9},{"cell_type":"markdown","metadata":{"id":"EAC0446D82144B56A6DD3B8C70D0A227","notebookId":"67f4d858eb20abf0847c425e","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"## 环境导入（包导入）"},{"cell_type":"code","metadata":{"id":"41B761C5C6CC45D5AA61B0DCAF5C0E9A","notebookId":"67f4d858eb20abf0847c425e","jupyter":{},"collapsed":false,"scrolled":true,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"import torch  # PyTorch深度学习框架\nfrom datasets import load_dataset, Dataset  # Hugging Face的数据集加载工具\nfrom transformers import (  # Hugging Face的转换器库\n    AutoModelForCausalLM,  # 自回归语言模型（用于生成文本）\n    AutoTokenizer,  # 自动分词器\n    AutoModelForMaskedLM,  # 掩码语言模型\n    get_linear_schedule_with_warmup,  # 学习率预热调度器\n    DataCollatorForLanguageModeling  # 用于MLM的数据整理器\n)\nfrom peft import LoraConfig, get_peft_model, TaskType  # 参数高效微调工具\nfrom torch.utils.data import DataLoader, random_split  # 数据加载相关工具\nfrom torch.optim import AdamW  # Adam优化器的变体\nfrom tqdm import tqdm  # 进度条工具\nimport os\nimport random\nimport numpy as np\nimport json\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sentence_transformers import SentenceTransformer\nimport logging\nfrom datetime import datetime\nimport json\nimport random\nimport re\nimport os\nfrom pathlib import Path\nfrom tqdm import tqdm\nfrom collections import defaultdict, Counter\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\nimport math\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport jieba\nimport pythainlp\nfrom pythainlp.corpus import thai_stopwords\nfrom pythainlp.tokenize import word_tokenize as thai_tokenize\nfrom pythainlp import pos_tag as thai_pos_tag\nfrom typing import List, Dict, Set\nimport langdetect\nfrom langdetect.lang_detect_exception import LangDetectException","outputs":[{"output_type":"stream","name":"stderr","text":"/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\nINFO:datasets:PyTorch version 2.5.1 available.\n"}],"execution_count":10},{"cell_type":"markdown","metadata":{"id":"B60060F8FC8E412791FDDCF4417A47AA","notebookId":"67f4d858eb20abf0847c425e","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"  ## 模型评估与优化  （确定评估指标）  \n  - 困惑度(Perplexity)  \n    - 评估模型在验证集上的表现  \n    - 监控模型是否过拟合  \n  - 领域适应性评估  \n    - 术语覆盖率：# term_coverage是术语覆盖率，计算response中包含的领域术语数量占总术语数量的比例  \n    - 术语密度：# term_density是术语密度，计算response中包含的领域术语数量占总token数量的比例  \n    - 响应质量：# response_quality是回复质量，计算response与prompt的相似度"},{"cell_type":"code","metadata":{"id":"C1FCBEF4872149AE823840E14ABE30DD","notebookId":"67f4d858eb20abf0847c425e","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"class DomainEvaluator:\n    def __init__(self, tokenizer, device):\n        self.tokenizer = tokenizer\n        self.device = device\n        # 加载sentence transformer用于计算文本相似度\n        self.sentence_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n        self.sentence_model.to(device)\n        \n        # 加载领域术语\n        with open(\"/home/mw/input/Arabic60526052/domain_terms_arabic.txt\", \"r\", encoding=\"utf-8\") as f:\n            self.domain_terms = [line.strip() for line in f if line.strip()]\n        \n    # def calculate_domain_perplexity(self, model, eval_dataloader):\n    #     \"\"\"计算领域数据的困惑度\"\"\"\n    #     model.eval()\n    #     total_loss = 0\n    #     total_tokens = 0\n        \n    #     with torch.no_grad():\n    #         for batch in eval_dataloader:\n    #             batch = {k: v.to(self.device) for k, v in batch.items()}\n    #             outputs = model(**batch)\n    #             total_loss += outputs.loss.item() * batch[\"input_ids\"].size(0)\n    #             total_tokens += batch[\"input_ids\"].ne(self.tokenizer.pad_token_id).sum().item()\n        \n    #     return torch.exp(torch.tensor(total_loss / total_tokens))\n\n    def calculate_domain_perplexity(self, model, eval_dataloader, sample_ratio):\n        \"\"\"计算领域数据的困惑度，使用随机采样\n        \n        Args:\n            model: 模型\n            eval_dataloader: 评估数据加载器\n            sample_ratio: 评估样本比例，默认为0.3（30%）\n        \"\"\"\n        model.eval()\n        total_loss = 0\n        total_tokens = 0\n        \n        # 获取数据集大小\n        dataset_size = len(eval_dataloader.dataset)\n        \n        # 计算需要采样的样本数量\n        sample_size = int(dataset_size * sample_ratio)\n        \n        # 创建随机采样器\n        indices = torch.randperm(dataset_size)[:sample_size]\n        sampler = torch.utils.data.SubsetRandomSampler(indices)\n    \n        # 创建新的数据加载器\n        eval_dataloader = torch.utils.data.DataLoader(\n            eval_dataloader.dataset,\n            batch_size=eval_dataloader.batch_size,\n            sampler=sampler,\n            num_workers=eval_dataloader.num_workers,\n            pin_memory=eval_dataloader.pin_memory\n        )\n        \n        with torch.no_grad():\n            for batch in eval_dataloader:\n                batch = {k: v.to(self.device) for k, v in batch.items()}\n                outputs = model(**batch)\n                total_loss += outputs.loss.item() * batch[\"input_ids\"].size(0)\n                total_tokens += batch[\"input_ids\"].ne(self.tokenizer.pad_token_id).sum().item()\n    \n        return torch.exp(torch.tensor(total_loss / total_tokens))\n    \n    def evaluate_domain_adaptation(self, model, texts, lang=None):\n        \"\"\"评估生成文本的领域适应性\"\"\"\n        model.eval()\n        metrics = {\n            \"term_coverage\": [],\n            \"term_density\": [],\n            \"response_quality\": []\n        }\n        \n        with torch.no_grad():\n            for text in texts:\n                # 构建提示\n                if lang:\n                    prompt = f\"请用{lang}语言回答以下问题：\\n{text}\"\n                else:\n                    prompt = text\n                \n                # 生成回复\n                inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(self.device)\n                outputs = model.generate(\n                    **inputs,\n                    max_length=800,\n                    num_return_sequences=1,\n                    do_sample=True,\n                    temperature=0.7,\n                    top_p=0.95\n                )\n                response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n                \n                # 计算评估指标\n                term_count = sum(1 for term in self.domain_terms if term.lower() in response.lower())\n                term_coverage = term_count / len(self.domain_terms) if self.domain_terms else 0\n                metrics[\"term_coverage\"].append(term_coverage)\n                \n                term_density = term_count / len(response) if response else 0\n                metrics[\"term_density\"].append(term_density)\n                \n                # 计算回复质量\n                response_embedding = self.sentence_model.encode([response])\n                prompt_embedding = self.sentence_model.encode([text])\n                similarity = cosine_similarity(response_embedding, prompt_embedding)[0][0]\n                metrics[\"response_quality\"].append(similarity)\n        \n        return {\n            \"avg_term_coverage\": np.mean(metrics[\"term_coverage\"]),\n            \"avg_term_density\": np.mean(metrics[\"term_density\"]),\n            \"avg_response_quality\": np.mean(metrics[\"response_quality\"])\n        }","outputs":[],"execution_count":11},{"cell_type":"markdown","metadata":{"id":"BCD356AFF2D84706BA757886E83858FB","notebookId":"67f4d858eb20abf0847c425e","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"## 数据处理  \n### 数据处理流程  \n- 数据清洗  \n- 去除HTML标签和特殊字符  \n- 规范化文本格式  \n- 过滤过长或过短的文本  \n- 数据转换  \n- 构建输入输出对  \n- 添加任务相关的指令提示"},{"cell_type":"code","metadata":{"id":"C6C4A0EBD2774FE88653F61F376D552F","notebookId":"67f4d858eb20abf0847c425e","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"import json\nimport gzip\nfrom pathlib import Path\nimport re\nfrom tqdm import tqdm\nimport random\nimport os\nimport pandas as pd\n\ndef clean_text(text):\n    \"\"\"清理文本内容\"\"\"\n    if not text:\n        return text\n    \n    # 正则表达式是一种文本匹配模式,下面详细解释每一步:\n    \n    # 1. 处理连续换行符\n    # re.sub()函数用于替换文本,接受3个参数:\n    # - 第1个参数 r'\\n+' 表示:\n    #   \\n 代表换行符\n    #   + 表示匹配1个或多个连续的换行符\n    # - 第2个参数 '\\n' 表示用单个换行符替换\n    # - 第3个参数是要处理的文本\n    # strip()去除文本首尾的空格\n    text = re.sub(r'\\n+', '\\n', text.strip())\n    \n    # 2. 处理连续空格\n    # r'\\s+' 表示:\n    # \\s 代表任意空白字符(空格、制表符等)\n    # + 表示匹配1个或多个连续的空白字符\n    # 用单个空格替换所有连续的空白字符\n    text = re.sub(r'\\s+', ' ', text)\n    \n    # 3. 移除HTML标签，正则表达式 r'<[^>]+>'\n    # 1) r'' 表示这是一个原始字符串,不会对反斜杠\\进行转义处理\n    # 2) < 就是匹配HTML标签的开始符号 <\n    # 3) [^>] 是一个字符集:\n    #    - [] 表示匹配其中的任意一个字符\n    #    - ^ 在[]内表示\"非\",即取反\n    #    - 所以[^>]表示匹配任何不是>的字符\n    # 4) + 表示\"一个或多个\",即重复前面的[^>]一次或多次\n    # 5) > 就是匹配HTML标签的结束符号 >\n    # \n    # 举例说明:\n    # 原文本: \"这是<p>一个段落</p>\"\n    # - <p> 会被匹配,因为它符合模式:<加上任意非>字符(这里是p)再加上>\n    # - </p> 也会被匹配,因为它符合模式:<加上任意非>字符(这里是/p)再加上>\n    # \n    # re.sub()会把所有匹配到的内容替换为空字符串'',所以最后变成:\n    # \"这是一个段落\"\n    text = re.sub(r'<[^>]+>', '', text)\n    return text.strip()\n\ndef process_item(item):\n    \"\"\"处理单条数据，统一不同数据集的格式\"\"\"\n    try:\n        # 检查数据格式并提取必要字段\n        if isinstance(item, dict):\n            # 常见格式：包含title和content的字典\n            if 'title' in item and 'content' in item:\n                title = clean_text(item['title'])\n                content = clean_text(item['content'])\n            # 其他可能的格式\n            elif 'text' in item:\n                # 如果只有text字段，尝试从文本中提取标题\n                text = clean_text(item['text'])\n                lines = text.split('\\n', 1)\n                if len(lines) > 1:\n                    title, content = lines\n                else:\n                    title = \"文章\"\n                    content = text\n            else:\n                return None\n        else:\n            return None\n            \n        # 检查文本长度\n        if len(content) < 50 or len(content) > 10000:\n            return None\n            \n        # 构建统一的训练格式\n        conversation = {\n            \"instruction\": f\"请生成一段带有阿拉伯专业术语的文本。\\n\\n标题: {title}\",\n            \"input\": \"\",\n            \"output\": content,\n            \"category\": item.get('labels', {}).get('pjwk_cates', \"general\")\n        }\n        \n        return conversation\n    except Exception as e:\n        print(f\"处理数据时出错: {e}\")\n        return None\n\ndef process_file(input_path, sample_ratio=0.1):\n    \"\"\"处理单个文件并随机抽样\"\"\"\n    processed_data = []\n    \n    try:\n        # 读取gzip文件\n        with gzip.open(input_path, 'rt', encoding='utf-8') as f:\n            # 首先读取所有行\n            lines = f.readlines()\n            \n            # 随机抽样\n            # 确保sample_size至少为1,避免抽样失败\n            sample_size = max(1, int(len(lines) * sample_ratio))\n            # 从lines列表中随机抽取sample_size条数据\n            sampled_lines = random.sample(lines, sample_size)\n            \n            # 处理抽样的数据\n            for line in tqdm(sampled_lines, desc=f\"处理文件 {Path(input_path).name}\"):\n                try:\n                    item = json.loads(line.strip())\n                    processed_item = process_item(item)\n                    if processed_item:\n                        processed_data.append(processed_item)\n                except json.JSONDecodeError:\n                    continue\n                except Exception as e:\n                    print(f\"处理数据时出错: {e}\")\n                    continue\n    except Exception as e:\n        print(f\"处理文件 {input_path} 时出错: {e}\")\n    \n    return processed_data\n\ndef process_all_datasets(base_dir, output_path, sample_ratio=0.25):\n    \"\"\"处理所有数据集并合并结果\"\"\"\n    all_processed_data = []\n    \n    # 递归查找所有.jsonl.gz文件\n    for root, _, files in os.walk(base_dir):\n        for file in files:\n            if file.endswith('.jsonl.gz'):\n                input_path = os.path.join(root, file)\n                print(f\"\\n处理文件: {input_path}\")\n                \n                # 处理单个文件\n                file_data = process_file(input_path, sample_ratio)\n                all_processed_data.extend(file_data)\n                \n                print(f\"从 {file} 中提取了 {len(file_data)} 条数据\")\n    \n    # 保存所有处理后的数据\n    output_path = Path(output_path)\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    \n    with open(output_path, 'w', encoding='utf-8') as f:\n        json.dump(all_processed_data, f, ensure_ascii=False, indent=2)\n    \n    print(f\"\\n处理完成！\")\n    print(f\"总共处理了 {len(all_processed_data)} 条数据\")\n    print(f\"数据已保存至: {output_path}\")\n    \n    # 输出数据集统计信息\n    categories = {}\n    for item in all_processed_data:\n        cat = item['category']\n        if isinstance(cat, dict):\n            cat = str(cat)\n        categories[cat] = categories.get(cat, 0) + 1\n    \n    print(\"\\n数据集类别分布:\")\n    for cat, count in categories.items():\n        print(f\"{cat}: {count} 条\")\n\ndef main():\n    # 设置随机种子以确保可重复性\n    random.seed(42)\n    \n    # 输入输出路径\n    base_dir = \"/home/mw/input/raw_arabic81628162/\"  # 原始数据集目录\n    output_file = \"/home/mw/input/Arabic60526052/combined_processed_data_arabic.json\"  # 输出文件路径\n\n    # 处理所有数据集\n    process_all_datasets(base_dir, output_file, sample_ratio=0.25)\n    \n    # 显示样例数据\n    with open(output_file, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n        print(\"\\n处理后的数据样例:\")\n        print(json.dumps(data[0], ensure_ascii=False, indent=2))\n\nif __name__ == \"__main__\":\n    main() ","outputs":[{"output_type":"stream","name":"stdout","text":"\n处理文件: /home/mw/input/raw_arabic81628162/阿拉伯part-677f75d865d8-001143.jsonl.gz\n"},{"output_type":"stream","name":"stderr","text":"处理文件 阿拉伯part-677f75d865d8-001143.jsonl.gz: 100%|██████████| 569725/569725 [00:08<00:00, 68272.43it/s]\n"},{"output_type":"stream","name":"stdout","text":"从 阿拉伯part-677f75d865d8-001143.jsonl.gz 中提取了 20976 条数据\n\n处理完成！\n总共处理了 20976 条数据\n数据已保存至: /home/mw/input/Arabic60526052/combined_processed_data_arabic.json\n\n数据集类别分布:\n{'level1': ['professional_field'], 'level2': ['technology']}: 10707 条\n{'level1': ['professional_field'], 'level2': ['finance']}: 2881 条\n{'level1': ['professional_field'], 'level2': ['law']}: 124 条\n{'level1': ['professional_field'], 'level2': ['academic']}: 6182 条\n{'level1': ['professional_field'], 'level2': ['patent']}: 619 条\n{'level1': ['professional_field'], 'level2': ['institutions']}: 371 条\n{'level1': ['professional_field'], 'level2': ['education']}: 92 条\n\n处理后的数据样例:\n{\n  \"instruction\": \"请生成一段带有阿拉伯专业术语的文本。\\n\\n标题: آخر الأخبار: ifor\",\n  \"input\": \"\",\n  \"output\": \"الأخبار التقنية أعلنت اليوم شركة إنفور، الرائدة في مجال توريد تطبيقات المؤسسية المتخصصة والقائمة على السحابة، عن مشاركتها واستعراضها لعروض حزمة CloudSuite…\",\n  \"category\": {\n    \"level1\": [\n      \"professional_field\"\n    ],\n    \"level2\": [\n      \"technology\"\n    ]\n  }\n}\n"}],"execution_count":12},{"cell_type":"markdown","metadata":{"id":"55B263E1736C42669141B301649AFB25","notebookId":"67f4d858eb20abf0847c425e","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"### 提取专业术语：如“操作系统”、“高血压”  \n注意事项：先撇去阿拉伯语语种的特性影响，如音节，再进行分词，这样的分词效果更好"},{"cell_type":"code","metadata":{"id":"71F767C0C0C1450091A46AC85134C88F","notebookId":"67f4d858eb20abf0847c425e","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"def extract_domain_terms_by_language(text: str, lang: str) -> List[str]:\n    \"\"\"使用语言专属工具提取领域术语\"\"\"\n    terms = []\n    if lang == 'ar':\n        try:\n            # 使用pyarabic进行基本处理\n            text = araby.strip_tashkeel(text)  # 移除变音符号\n            words = araby.tokenize(text)\n            # 提取可能的术语（长度大于3的词）\n            terms = [w for w in words if len(w) > 3 and not any(c.isdigit() for c in w)]\n        except Exception as e:\n            print(f\"阿拉伯语处理失败: {e}\")\n    \n    return terms\n\ndef extract_domain_terms(domain_texts: List[str], general_texts: List[str] = None, top_n: int = 2000) -> List[str]:\n    \"\"\"使用改进的多语言分词方法提取领域术语\"\"\"\n    print(\"开始提取领域术语...\")\n    \n    # 按语言分组处理文本\n    lang_texts = defaultdict(list)\n    for text in domain_texts:\n        lang = detect_language(text)\n        if lang != 'unknown':\n            lang_texts[lang].append(text)\n    \n    print(\"\\n文本语言分布:\")\n    for lang, texts in lang_texts.items():\n        print(f\"- {lang}: {len(texts)} 条\")\n    \n    # 分语言处理并提取术语\n    all_terms = []\n    for lang, texts in lang_texts.items():\n        print(f\"\\n处理{lang}语言文本...\")\n        \n        # 使用语言专属工具提取术语\n        lang_terms = set()\n        for text in tqdm(texts, desc=f\"处理{lang}语言文本\"):\n            terms = extract_domain_terms_by_language(text, lang)\n            lang_terms.update(terms)\n        \n        # 过滤和排序术语\n        lang_terms = list(lang_terms)\n        # 计算每个术语在文本中出现的频率\n        # 1. 遍历每个文本\n        # 2. 对每个文本重新提取术语\n        # 3. 使用Counter统计所有术语的频率\n        term_freq = Counter(t for text in texts for t in extract_domain_terms_by_language(text, lang))\n        \n        # 对术语列表进行排序:\n        # 1. 首要排序依据是术语出现频率(term_freq[x])\n        # 2. 次要排序依据是术语长度(len(x))\n        # reverse=True表示按降序排列,即频率高的和长度长的排在前面\n        lang_terms.sort(key=lambda x: (term_freq[x], len(x)), reverse=True)\n        \n        # 根据语言数量平均分配术语数量配额\n        # 1. top_n是总的期望术语数量\n        # 2. len(lang_texts)是语言种类数\n        # 3. 对每种语言,只取配额内的高频长术语\n        # //是整除运算符,用于计算每种语言分配的术语数量配额\n        # 例如:如果top_n=2000,有4种语言,则每种语言分配2000//4=500个术语\n        # 这里的:是切片操作符,表示从列表开头取到指定位置\n        # //是整除运算符,例如10//3=3\n        # 所以lang_terms[:top_n // len(lang_texts)]表示:\n        # 1. 先计算top_n除以语言数量的整除结果n\n        # 2. 然后从lang_terms列表中取前n个元素\n        selected_terms = lang_terms[:top_n // len(lang_texts)]\n        all_terms.extend(selected_terms)\n        \n        print(f\"{lang}语言提取了 {len(selected_terms)} 个术语\")\n        if selected_terms:\n            print(f\"{lang}语言术语示例:\")\n            for term in selected_terms[:5]:\n                print(f\"  - {term}\")\n    \n    return all_terms\n","outputs":[],"execution_count":13},{"cell_type":"markdown","metadata":{"id":"C1D34A3BEB324C4493010B2A4D42CDC0","notebookId":"67f4d858eb20abf0847c425e","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"### 数据集构建  \n- *将原始数据转换为指令微调格式*  \n- 训练集和验证集划分  \n- 数据格式标准化"},{"cell_type":"code","metadata":{"id":"95B7788B5B584CA19C9AF36347D7B949","notebookId":"67f4d858eb20abf0847c425e","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"\ndef load_and_prepare_data(json_path, tokenizer, max_length=512, val_ratio=0.1):\n    \"\"\"加载并预处理微调数据（标准LLM微调流程）\"\"\"\n    with open(json_path, 'r', encoding='utf-8') as f:\n        raw_data = json.load(f)\n    \n    # 记录每个样本中\"### Response:\\n\"的字符位置\n    formatted_texts = []\n    response_char_positions = []\n    for item in raw_data:\n        prompt = item['prompt']\n        output = item['output']\n        formatted_text = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n{output}\"\n        formatted_texts.append(formatted_text)\n        # 找到\"### Response:\\n\"的字符位置\n        response_start_char = formatted_text.find(\"### Response:\\n\") + len(\"### Response:\\n\")\n        response_char_positions.append(response_start_char)\n    \n    # 分词处理\n    tokenized_data = tokenizer(\n        formatted_texts,\n        max_length=max_length,\n        truncation=True,\n        padding=\"max_length\",  # 修改：使用max_length填充\n        add_special_tokens=True,\n        return_offsets_mapping=True  # 需要字符到token的映射\n    )\n    \n    # 准备标签\n    labels = []\n    for i in range(len(tokenized_data[\"input_ids\"])):\n        input_ids = tokenized_data[\"input_ids\"][i]\n        offsets = tokenized_data[\"offset_mapping\"][i]\n        \n        # 通过字符位置找到response起始的token索引\n        response_start_token = None\n        for token_idx, (char_start, char_end) in enumerate(offsets):\n            if char_start >= response_char_positions[i]:\n                response_start_token = token_idx\n                break\n        \n        # 如果未找到（例如被截断），则设为整个序列\n        if response_start_token is None:\n            response_start_token = len(input_ids)\n        \n        # 创建label：只保留response部分的token_id\n        label = [-100] * len(input_ids)\n        label[response_start_token:] = input_ids[response_start_token:]\n        labels.append(label)\n    \n    # 移除offset_mapping（不再需要）\n    tokenized_data.pop(\"offset_mapping\")\n    \n    # 创建数据集\n    dataset = Dataset.from_dict({\n        \"input_ids\": tokenized_data[\"input_ids\"],\n        \"attention_mask\": tokenized_data[\"attention_mask\"],\n        \"labels\": labels\n    })\n    \n    # 划分训练验证集\n    split_dataset = dataset.train_test_split(test_size=val_ratio, seed=42)\n    \n    # 转换为PyTorch张量格式\n    def set_format(ds):\n        ds.set_format(type='torch', columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n        return ds\n    \n    train_dataset = set_format(split_dataset[\"train\"])\n    val_dataset = set_format(split_dataset[\"test\"])\n    \n    # 修正后的collate_fn - 确保所有序列长度一致\n    def collate_fn(batch):\n        # 将batch中的每个tensor转为列表以便处理\n        input_ids = [item[\"input_ids\"].tolist() for item in batch]\n        attention_mask = [item[\"attention_mask\"].tolist() for item in batch]\n        labels = [item[\"labels\"].tolist() for item in batch]\n        \n        # 找出最大长度\n        max_len = max(len(ids) for ids in input_ids)\n        \n        # 进行右侧填充\n        for i in range(len(batch)):\n            pad_len = max_len - len(input_ids[i])\n            if pad_len > 0:\n                input_ids[i] += [tokenizer.pad_token_id] * pad_len\n                attention_mask[i] += [0] * pad_len\n                labels[i] += [-100] * pad_len  # 使用-100填充标签\n        \n        # 转换回tensor\n        result = {\n            \"input_ids\": torch.tensor(input_ids),\n            \"attention_mask\": torch.tensor(attention_mask),\n            \"labels\": torch.tensor(labels)\n        }\n        \n        return result\n    \n    # 创建DataLoader\n    train_dataloader = DataLoader(\n        train_dataset,\n        batch_size=4,\n        shuffle=True,\n        collate_fn=collate_fn\n    )\n    val_dataloader = DataLoader(\n        val_dataset,\n        batch_size=4,\n        collate_fn=collate_fn\n    )\n    \n    return train_dataloader, val_dataloader","outputs":[],"execution_count":14},{"cell_type":"markdown","metadata":{"id":"2FD7BFC2F7044E51BD8F92C04E786E9F","notebookId":"67f4d858eb20abf0847c425e","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"## 编码：数据处理成适合MLM的格式  \n\n#### 功能描述  \n\n该函数用于加载和预处理用于大语言模型微调的数据。它将原始JSON格式的训练数据转换为适合模型训练的格式,并创建训练和验证数据加载器。  \n\n#### 参数说明  \n\n- `json_path` (str): 输入JSON文件的路径  \n- `tokenizer`: 用于文本分词的tokenizer对象  \n- `max_length` (int): 序列的最大长度,默认为512  \n- `val_ratio` (float): 验证集比例,默认为0.1  \n\n#### 处理流程  \n\n1. **数据加载与格式化**  \n   - 从JSON文件加载原始数据  \n   - 将每个样本格式化为\"### Instruction:\\n{prompt}\\n\\n### Response:\\n{output}\"的形式  \n   - 记录每个样本中\"### Response:\\n\"的字符位置  \n\n2. **分词处理**  \n   - 使用tokenizer对文本进行分词  \n   - 设置最大长度并进行截断  \n   - 使用max_length进行填充  \n   - 获取字符到token的映射信息  \n\n3. **标签准备**  \n   - 通过字符位置找到response起始的token索引  \n   - 创建标签序列:  \n     - response之前的token标记为-100(忽略)  \n     - response部分保留原token_id  \n\n4. **数据集创建与划分**  \n   - 使用Huggingface的Dataset类创建数据集  \n   - 按照设定比例划分训练集和验证集  \n   - 将数据格式转换为PyTorch张量  \n\n5. **DataLoader创建**  \n   - 实现collate_fn确保batch中序列长度一致  \n   - 创建训练和验证数据加载器  \n   - 设置batch_size为4  \n\n#### 返回值  \n\n返回一个元组,包含:  \n- train_dataloader: 训练数据加载器  \n- val_dataloader: 验证数据加载器"},{"cell_type":"code","metadata":{"id":"C8DDF6050266493EB468A72BFB7091EE","notebookId":"67f4d858eb20abf0847c425e","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"import json\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer\nfrom torch.utils.data import DataLoader\n\ndef load_and_prepare_data(json_path, tokenizer, max_length=512, val_ratio=0.1):\n    \"\"\"加载并预处理微调数据（标准LLM微调流程）\"\"\"\n    with open(json_path, 'r', encoding='utf-8') as f:\n        raw_data = json.load(f)\n    \n    # 记录每个样本中\"### Response:\\n\"的字符位置\n    formatted_texts = []\n    response_char_positions = []\n    for item in raw_data:\n        prompt = item['prompt']\n        output = item['output']\n        formatted_text = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n{output}\"\n        formatted_texts.append(formatted_text)\n        # 找到\"### Response:\\n\"的字符位置\n        response_start_char = formatted_text.find(\"### Response:\\n\") + len(\"### Response:\\n\")\n        response_char_positions.append(response_start_char)\n    \n    # 分词处理\n    tokenized_data = tokenizer(\n        formatted_texts,\n        max_length=max_length,\n        truncation=True,\n        padding=\"max_length\",  # 修改：使用max_length填充\n        add_special_tokens=True,\n        return_offsets_mapping=True  # 需要字符到token的映射\n    )\n    \n    # 准备标签\n    labels = []\n    for i in range(len(tokenized_data[\"input_ids\"])):\n        input_ids = tokenized_data[\"input_ids\"][i]\n        offsets = tokenized_data[\"offset_mapping\"][i]\n        \n        # 通过字符位置找到response起始的token索引\n        response_start_token = None\n        for token_idx, (char_start, char_end) in enumerate(offsets):\n            if char_start >= response_char_positions[i]:\n                response_start_token = token_idx\n                break\n        \n        # 如果未找到（例如被截断），则设为整个序列\n        if response_start_token is None:\n            response_start_token = len(input_ids)\n        \n        # 创建label：只保留response部分的token_id\n        label = [-100] * len(input_ids)\n        label[response_start_token:] = input_ids[response_start_token:]\n        labels.append(label)\n    \n    # 移除offset_mapping（不再需要）\n    tokenized_data.pop(\"offset_mapping\")\n    \n    # 创建数据集\n    dataset = Dataset.from_dict({\n        \"input_ids\": tokenized_data[\"input_ids\"],\n        \"attention_mask\": tokenized_data[\"attention_mask\"],\n        \"labels\": labels\n    })\n    \n    # 划分训练验证集\n    split_dataset = dataset.train_test_split(test_size=val_ratio, seed=42)\n    \n    # 转换为PyTorch张量格式\n    def set_format(ds):\n        ds.set_format(type='torch', columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n        return ds\n    \n    train_dataset = set_format(split_dataset[\"train\"])\n    val_dataset = set_format(split_dataset[\"test\"])\n    \n    # 修正后的collate_fn - 确保所有序列长度一致\n    def collate_fn(batch):\n        # 将batch中的每个tensor转为列表以便处理\n        input_ids = [item[\"input_ids\"].tolist() for item in batch]\n        attention_mask = [item[\"attention_mask\"].tolist() for item in batch]\n        labels = [item[\"labels\"].tolist() for item in batch]\n        \n        # 找出最大长度\n        max_len = max(len(ids) for ids in input_ids)\n        \n        # 进行右侧填充\n        for i in range(len(batch)):\n            pad_len = max_len - len(input_ids[i])\n            if pad_len > 0:\n                input_ids[i] += [tokenizer.pad_token_id] * pad_len\n                attention_mask[i] += [0] * pad_len\n                labels[i] += [-100] * pad_len  # 使用-100填充标签\n        \n        # 转换回tensor\n        result = {\n            \"input_ids\": torch.tensor(input_ids),\n            \"attention_mask\": torch.tensor(attention_mask),\n            \"labels\": torch.tensor(labels)\n        }\n        \n        return result\n    \n    # 创建DataLoader\n    train_dataloader = DataLoader(\n        train_dataset,\n        batch_size=4,\n        shuffle=True,\n        collate_fn=collate_fn\n    )\n    val_dataloader = DataLoader(\n        val_dataset,\n        batch_size=4,\n        collate_fn=collate_fn\n    )\n    \n    return train_dataloader, val_dataloader\n\n","outputs":[],"execution_count":15},{"cell_type":"markdown","metadata":{"id":"0D72CBF74DF54A5283AEABE0DEC2E096","notebookId":"67f4d858eb20abf0847c425e","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"## 模型加载与训练——deepseek1.5B&开始训练  \n  1. 模型准备  \n     - 加载DeepSeek基础模型  \n     - 配置tokenizer  \n     - 设置LoRA参数  \n  2. 训练配置  \n     - 学习率设置  \n     - 批次大小选择  \n     - 训练轮次确定  \n     - 优化器选择  \n  3. 训练过程  \n     - 梯度更新  \n     - 学习率调度  \n     - 模型保存  \n     - 训练监控"},{"cell_type":"code","metadata":{"id":"72EA90D8E9F245E08591979150685E1C","notebookId":"67f4d858eb20abf0847c425e","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"\n\n# 设置日志\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler(f'training_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'),\n        logging.StreamHandler()\n    ]\n)\nlogger = logging.getLogger(__name__)\n\n# 设置随机种子以确保结果可重现\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\n\ndef find_optimal_lora_config(model, train_dataloader, val_dataloader, device, evaluator):\n    \"\"\"搜索最优的LoRA配置，使用小数据集快速搜索\"\"\"\n    configs = [\n        {\"r\": 16, \"alpha\": 64},\n        {\"r\": 8, \"alpha\": 32},\n        {\"r\": 4, \"alpha\": 16}\n    ]\n    \n    best_perplexity = float('inf')\n    best_config = None\n    \n    # 从训练集和验证集中各取100条数据创建小数据集\n    small_train_data = []\n    small_val_data = []\n    \n    # 收集小训练集\n    train_iter = iter(train_dataloader)\n    for _ in range(min(25, len(train_dataloader))):  # 25个batch * 4 = 100条数据\n        try:\n            batch = next(train_iter)\n            small_train_data.append(batch)\n        except StopIteration:\n            break\n    \n    # 收集小验证集\n    val_iter = iter(val_dataloader)\n    for _ in range(min(25, len(val_dataloader))):\n        try:\n            batch = next(val_iter)\n            small_val_data.append(batch)\n        except StopIteration:\n            break\n    \n    logger.info(f\"创建了小数据集用于配置搜索：训练集 {len(small_train_data)} 批次，验证集 {len(small_val_data)} 批次\")\n    \n    for config in configs:\n        logger.info(f\"\\n测试LoRA配置: r={config['r']}, alpha={config['alpha']}\")\n        \n        # 配置LoRA\n        lora_config = LoraConfig(\n            r=config['r'],\n            lora_alpha=config['alpha'],\n            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n            lora_dropout=0.05,\n            bias=\"none\",\n            task_type=TaskType.CAUSAL_LM\n        )\n        \n        # 创建PEFT模型\n        peft_model = get_peft_model(model, lora_config)\n        \n        # 快速训练和评估\n        optimizer = AdamW(peft_model.parameters(), lr=2e-4)\n        peft_model.train()\n        \n        # 在小训练集上训练\n        for _ in range(2):  # 只训练2个epoch\n            for batch in small_train_data:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                outputs = peft_model(**batch)\n                loss = outputs.loss\n                loss.backward()\n                optimizer.step()\n                optimizer.zero_grad()\n        \n        # 在小验证集上评估\n        peft_model.eval()\n        total_loss = 0\n        total_tokens = 0\n        with torch.no_grad():\n            for batch in small_val_data:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                outputs = peft_model(**batch)\n                total_loss += outputs.loss.item() * batch[\"input_ids\"].size(0)\n                total_tokens += batch[\"input_ids\"].ne(evaluator.tokenizer.pad_token_id).sum().item()\n        # 计算困惑度，原理是计算验证集的损失除以验证集的总token数量，然后取指数，目的是衡量模型的困惑度\n        perplexity = torch.exp(torch.tensor(total_loss / total_tokens))\n        logger.info(f\"配置性能 - 困惑度: {perplexity:.2f}\")\n        \n        if perplexity < best_perplexity:\n            best_perplexity = perplexity\n            best_config = config\n            logger.info(f\"找到新的最佳配置！\")\n    \n    return best_config\n\ndef convert_metrics_to_json_serializable(metrics):\n    \"\"\"将指标转换为JSON可序列化的格式\"\"\"\n    if isinstance(metrics, dict):\n        return {k: convert_metrics_to_json_serializable(v) for k, v in metrics.items()}\n    elif isinstance(metrics, list):\n        return [convert_metrics_to_json_serializable(v) for v in metrics]\n    elif isinstance(metrics, (torch.Tensor, np.ndarray)):\n        return metrics.item() if metrics.size == 1 else metrics.tolist()\n    elif isinstance(metrics, (int, float, str, bool)):\n        return metrics\n    elif metrics is None:\n        return None\n    else:\n        return str(metrics)\n\ndef main():\n    # 设置随机种子\n    set_seed(42)\n    \n    # 检查CUDA\n    assert torch.cuda.is_available(), \"需要CUDA支持\"\n    device = torch.device(\"cuda:0\")\n    \n    print(os.environ.get('HF_ENDPOINT'))\n    print(os.environ.get('HF_ENDPOINT'))\n    print(os.environ.get('HF_ENDPOINT'))\n\n    # 加载模型和分词器\n    model_path = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n    model = AutoModelForCausalLM.from_pretrained(\n        model_path,\n        trust_remote_code=True,\n        torch_dtype=torch.float16,\n    \n        \n    ).to(device)\n    \n    tokenizer = AutoTokenizer.from_pretrained(\n        model_path,\n        trust_remote_code=True,\n    \n    )\n    tokenizer.pad_token = tokenizer.eos_token\n    \n    train_dataloader, val_dataloader = load_and_prepare_data(\"/home/mw/input/Arabic60526052/lora_training_data_arabic.json\", tokenizer)\n    \n\n    # 初始化评估器\n    evaluator = DomainEvaluator(tokenizer, device)\n    \n    # # 寻找最优LoRA配置\n    # logger.info(\"开始寻找最优LoRA配置...\")\n    # best_config = find_optimal_lora_config(model, train_dataloader, val_dataloader, device, evaluator)\n    # logger.info(f\"找到最优LoRA配置: r={best_config['r']}, alpha={best_config['alpha']}\")\n    \n    # # 使用最优配置创建LoRA模型\n    # lora_config = LoraConfig(\n    #     r=best_config['r'],\n    #     lora_alpha=best_config['alpha'],\n    #     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    #     lora_dropout=0.1,\n    #     bias=\"none\",\n    #     task_type=TaskType.CAUSAL_LM\n    # )\n    \n    lora_config = LoraConfig(\n        r=4,\n        lora_alpha=16,\n        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n        lora_dropout=0.1,\n        bias=\"none\",\n        task_type=TaskType.CAUSAL_LM\n    )\n\n    model = get_peft_model(model, lora_config)\n    model.print_trainable_parameters()\n    \n    # 训练配置\n    optimizer = AdamW(model.parameters(), lr=5e-4)\n    num_epochs = 2\n    num_training_steps = len(train_dataloader) * num_epochs\n    lr_scheduler = get_linear_schedule_with_warmup(\n        optimizer, \n        num_warmup_steps=200,\n        num_training_steps=num_training_steps\n    )\n    \n    # 从domain_terms_arabic.txt构建评估提示\n    with open(\"/home/mw/input/Arabic60526052/domain_terms_arabic.txt\", \"r\", encoding=\"utf-8\") as f:\n        domain_terms = [line.strip() for line in f if line.strip()]\n    \n    # 构建基于领域术语的评估提示\n    unlabeled_eval_prompts = {\n        \"ar\": [\n            f\"请解释阿拉伯语中'{term}'这个术语的含义和用法。\" for term in random.sample(domain_terms, 5)\n        ] + [\n            f\"请用阿拉伯语写一段话，包含以下术语：{', '.join(random.sample(domain_terms, 3))}\",\n            f\"在技术领域中，'{random.choice(domain_terms)}'和'{random.choice(domain_terms)}'这两个术语有什么联系？\",\n            f\"请用阿拉伯语描述'{random.choice(domain_terms)}'在现代技术发展中的应用。\",\n            f\"请生成一段带有阿拉伯专业术语的文本\",\n            f\"请生成一段带有阿拉伯专业术语的文本\",\n            f\"请生成一段带有阿拉伯专业术语的文本\",\n            f\"请生成一段带有阿拉伯专业术语的文本\",\n            f\"请生成一段带有阿拉伯专业术语的文本\",\n            f\"请生成一段带有阿拉伯专业术语的文本\",\n            f\"请生成一段带有阿拉伯专业术语的文本\",\n            f\"请生成一段带有阿拉伯专业术语的文本\"\n        ]\n    }\n    \n    # 训练循环\n    best_metrics = {\n        \"val_perplexity\": float('inf'),\n        \"domain_adaptation\": 0\n    }\n    metrics_log = []\n    eval_steps = 1000  # 每1000步评估一次\n    best_model_path = \"/home/mw/input/Arabic314891489/deepseek-lora-best\"\n    \n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0\n        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\")\n        print(len(train_dataloader))\n        for step, batch in enumerate(progress_bar):\n            batch = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(**batch)\n            loss = outputs.loss\n            total_loss += loss.item()\n            \n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n            \n            progress_bar.set_postfix({\"loss\": loss.item()})\n            \n            # 评估\n            if step % eval_steps == 0 and step > 0:\n                model.eval()\n                \n                # 1. 计算验证集困惑度\n                val_perplexity = evaluator.calculate_domain_perplexity(model, val_dataloader, 0.3)\n                \n                # 2. 评估领域适应性\n                domain_metrics = {}\n                for lang, prompts in unlabeled_eval_prompts.items():\n                    metrics = evaluator.evaluate_domain_adaptation(model, prompts, lang)\n                    domain_metrics[lang] = metrics\n                \n                # 3. 计算综合指标\n                avg_domain_score = np.mean([\n                    m[\"avg_term_coverage\"] * 0.4 +\n                    m[\"avg_term_density\"] * 0.3 +\n                    m[\"avg_response_quality\"] * 0.3\n                    for m in domain_metrics.values()\n                ])\n                \n                # 记录当前学习率\n                current_lr = optimizer.param_groups[0][\"lr\"]\n                \n                metrics = {\n                    \"epoch\": epoch + 1,\n                    \"step\": step,\n                    \"val_perplexity\": val_perplexity.item() if isinstance(val_perplexity, torch.Tensor) else float(val_perplexity),\n                    \"domain_adaptation_score\": float(avg_domain_score),\n                    \"learning_rate\": float(current_lr),\n                    \"domain_metrics\": convert_metrics_to_json_serializable(domain_metrics)\n                }\n                metrics_log.append(metrics)\n                \n                logger.info(f\"\\n验证集困惑度: {val_perplexity:.4f}\")\n                logger.info(f\"领域适应性得分: {avg_domain_score:.4f}\")\n                logger.info(f\"当前学习率: {current_lr:.6f}\")\n                \n                # 4. 保存最佳模型\n                combined_score = avg_domain_score/val_perplexity\n                if combined_score > best_metrics[\"domain_adaptation\"]/best_metrics[\"val_perplexity\"]:\n                    best_metrics[\"val_perplexity\"] = float(val_perplexity)\n                    best_metrics[\"domain_adaptation\"] = float(avg_domain_score)\n                    model.save_pretrained(best_model_path)\n                    logger.info(f\"保存新的最佳模型！困惑度={val_perplexity:.4f}, 领域得分={avg_domain_score:.4f}\")\n                \n                model.train()\n\n        # 每个epoch结束保存检查点\n        checkpoint_path = f\"/home/mw/input/Arabic314891489/deepseek-lora-checkpoint-{epoch+1}\"\n        try:\n            model.save_pretrained(checkpoint_path)\n            logger.info(f\"已保存Epoch {epoch+1}检查点\")\n        except Exception as e:\n            logger.error(f\"保存检查点时出错: {e}\")\n        \n        avg_loss = total_loss / len(train_dataloader)\n        logger.info(f\"Epoch {epoch+1} 平均损失: {avg_loss:.4f}\")\n    \n    # 保存最终模型和训练指标\n    try:\n        model.save_pretrained(\"/home/mw/input/Arabic314891489/deepseek-lora-final\")\n        with open('training_metrics.json', 'w', encoding='utf-8') as f:\n            json.dump(metrics_log, f, ensure_ascii=False, indent=2)\n        logger.info(\"训练完成！已保存最终模型和训练指标\")\n    except Exception as e:\n        logger.error(f\"保存最终结果时出错: {e}\")\n\n# 19. 主程序入口\nif __name__ == \"__main__\":\n    # 打印环境信息\n    logger.info(f\"PyTorch version: {torch.__version__}\")\n    logger.info(f\"CUDA available: {torch.cuda.is_available()}\")\n    logger.info(f\"CUDA version: {torch.version.cuda}\")\n    logger.info(f\"GPU count: {torch.cuda.device_count()}\")\n    logger.info(f\"Current GPU: {torch.cuda.current_device()}\")\n    logger.info(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n    \n    main()","outputs":[{"output_type":"stream","name":"stderr","text":"INFO:__main__:PyTorch version: 2.5.1+cu124\nINFO:__main__:CUDA available: True\nINFO:__main__:CUDA version: 12.4\nINFO:__main__:GPU count: 1\nINFO:__main__:Current GPU: 0\nINFO:__main__:GPU name: Tesla V100-SXM2-32GB\n"},{"output_type":"stream","name":"stdout","text":"https://hf-mirror.com\nhttps://hf-mirror.com\nhttps://hf-mirror.com\n"},{"output_type":"stream","name":"stderr","text":"INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda:0\nINFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n"},{"output_type":"stream","name":"stdout","text":"trainable params: 1,089,536 || all params: 1,778,177,536 || trainable%: 0.0613\n"},{"output_type":"stream","name":"stderr","text":"\rEpoch 1:   0%|          | 0/4719 [00:00<?, ?it/s]"},{"output_type":"stream","name":"stdout","text":"4719\n"},{"output_type":"stream","name":"stderr","text":"Epoch 1:  21%|██        | 1000/4719 [05:51<21:48,  2.84it/s, loss=5.76]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 54.01it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 112.76it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 77.35it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 103.74it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 85.26it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 110.00it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 91.14it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 122.04it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 92.96it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 114.09it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 79.93it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 116.76it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 86.31it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 112.99it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 86.14it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 103.61it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 105.46it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 88.22it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 91.66it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 113.27it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 86.23it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 119.78it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 92.29it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 126.54it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 90.21it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 119.81it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 85.34it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 119.53it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 86.61it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 113.63it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 93.32it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 119.12it/s]\nINFO:__main__:\n验证集困惑度: 1.0179\nINFO:__main__:领域适应性得分: 0.1798\nINFO:__main__:当前学习率: 0.000457\nINFO:__main__:保存新的最佳模型！困惑度=1.0179, 领域得分=0.1798\nEpoch 1:  42%|████▏     | 2000/4719 [23:15<15:58,  2.84it/s, loss=6.56]     Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 80.57it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 110.23it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 73.89it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 111.79it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 87.53it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 114.01it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 78.08it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 110.25it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 82.29it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 118.14it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 87.81it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 118.60it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 76.93it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 115.95it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 82.86it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 114.86it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 76.64it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 103.52it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 86.13it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 114.85it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 77.79it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 110.31it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 96.56it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 119.46it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 84.31it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 114.69it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 79.83it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 95.87it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 78.02it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 113.41it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 81.67it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 111.36it/s]\nINFO:__main__:\n验证集困惑度: 1.0169\nINFO:__main__:领域适应性得分: 0.1674\nINFO:__main__:当前学习率: 0.000403\nEpoch 1:  64%|██████▎   | 3000/4719 [40:44<10:04,  2.84it/s, loss=4.98]     Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 81.04it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 120.56it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 94.96it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 115.92it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 80.14it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 117.34it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 82.18it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 120.11it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 79.55it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 107.84it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 75.15it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 121.42it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 72.36it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 101.41it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 76.28it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 117.72it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 82.56it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 110.39it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 72.70it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 114.67it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 77.00it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 108.43it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 74.21it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 110.53it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 76.48it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 116.45it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 83.76it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 115.99it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 83.73it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 117.87it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 82.15it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 115.81it/s]\nINFO:__main__:\n验证集困惑度: 1.0171\nINFO:__main__:领域适应性得分: 0.1774\nINFO:__main__:当前学习率: 0.000348\nEpoch 1:  85%|████████▍ | 4000/4719 [58:12<04:12,  2.84it/s, loss=6.63]    Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 81.36it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 110.20it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 96.08it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 119.70it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 82.02it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 93.18it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 71.79it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 114.44it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 87.03it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 118.82it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 90.58it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 120.33it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 88.31it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 120.06it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 88.30it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 119.71it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 84.84it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 103.36it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 86.54it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 119.07it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 78.59it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 75.21it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 81.50it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 112.69it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 84.02it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 118.12it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 79.94it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 98.05it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 85.74it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 111.84it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 81.04it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 106.10it/s]\nINFO:__main__:\n验证集困惑度: 1.0164\nINFO:__main__:领域适应性得分: 0.1523\nINFO:__main__:当前学习率: 0.000294\nEpoch 1: 100%|██████████| 4719/4719 [1:14:03<00:00,  1.06it/s, loss=5.83]    \nINFO:__main__:已保存Epoch 1检查点\nINFO:__main__:Epoch 1 平均损失: 5.9745\nEpoch 2:   0%|          | 0/4719 [00:00<?, ?it/s]"},{"output_type":"stream","name":"stdout","text":"4719\n"},{"output_type":"stream","name":"stderr","text":"Epoch 2:  21%|██        | 1000/4719 [05:52<21:52,  2.83it/s, loss=5.57]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 81.39it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 112.48it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 79.86it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 123.97it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 76.27it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 113.56it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 84.00it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 118.17it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 87.04it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 117.92it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 80.06it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 114.26it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 77.83it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 115.27it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 85.70it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 111.62it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 82.68it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 115.66it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 92.15it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 123.97it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 77.51it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 115.62it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 81.90it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 117.32it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 82.63it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 116.01it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 84.86it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 108.48it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 68.71it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 119.86it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 84.68it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 102.49it/s]\nINFO:__main__:\n验证集困惑度: 1.0164\nINFO:__main__:领域适应性得分: 0.1710\nINFO:__main__:当前学习率: 0.000201\nEpoch 2:  42%|████▏     | 2000/4719 [23:20<16:00,  2.83it/s, loss=5.04]     Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 82.94it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 115.91it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 80.40it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 114.26it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 84.63it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 116.12it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 75.44it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 112.14it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 87.73it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 114.16it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 83.42it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 119.81it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 81.14it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 111.58it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 81.73it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 116.47it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 76.94it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 106.28it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 85.11it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 114.54it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 75.26it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 114.55it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 77.43it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 112.57it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 79.93it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 118.64it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 84.27it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 86.98it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 79.28it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 112.37it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 82.21it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 110.76it/s]\nINFO:__main__:\n验证集困惑度: 1.0159\nINFO:__main__:领域适应性得分: 0.1668\nINFO:__main__:当前学习率: 0.000147\nEpoch 2:  64%|██████▎   | 3000/4719 [40:48<10:04,  2.84it/s, loss=6.06]     Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 73.67it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 113.01it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 89.55it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 118.05it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 74.80it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 110.78it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 77.76it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 105.08it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 88.08it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 114.99it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 80.15it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 103.39it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 74.97it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 101.40it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 76.07it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 111.92it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 78.46it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 112.52it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 79.55it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 96.33it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 74.35it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 107.85it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 85.86it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 119.65it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 83.39it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 114.02it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 96.18it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 112.74it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 79.57it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 114.01it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 89.31it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 108.40it/s]\nINFO:__main__:\n验证集困惑度: 1.0164\nINFO:__main__:领域适应性得分: 0.1654\nINFO:__main__:当前学习率: 0.000093\nEpoch 2:  85%|████████▍ | 4000/4719 [58:21<04:13,  2.84it/s, loss=4.95]     Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 69.02it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 105.83it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 87.65it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 114.92it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 81.91it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 120.72it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 89.85it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 121.11it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 81.62it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 116.71it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 90.94it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 119.42it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 82.57it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 108.56it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 79.58it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 111.29it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 76.98it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 113.18it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 100.74it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 116.41it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 92.42it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 118.64it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 82.13it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 120.20it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 88.18it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 107.10it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 80.99it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 120.40it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 80.05it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 113.64it/s]\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 88.21it/s]\n\nBatches: 100%|██████████| 1/1 [00:00<00:00, 118.56it/s]\nINFO:__main__:\n验证集困惑度: 1.0164\nINFO:__main__:领域适应性得分: 0.1681\nINFO:__main__:当前学习率: 0.000039\nEpoch 2: 100%|██████████| 4719/4719 [1:14:12<00:00,  1.06it/s, loss=5.62]    \nINFO:__main__:已保存Epoch 2检查点\nINFO:__main__:Epoch 2 平均损失: 5.5541\nINFO:__main__:训练完成！已保存最终模型和训练指标\n"}],"execution_count":16},{"cell_type":"markdown","metadata":{"id":"A1051E10173A4EC09186F511A1B8416A","notebookId":"67f4d858eb20abf0847c425e","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"## 模型的更新（权重合并）与使用  \n## 注意，这里使用的是best权重，无需完整训练一个epoch也可加载出来best权重  \n\n这里加载“/home/mw/input/Arabic314891489/deepseek-lora-best”目录下的最佳模型  \n观察该模型权重性能  \n\n#### 先执行专业词解释，后执行文本生成任务"},{"cell_type":"code","metadata":{"id":"C5CE0664672543BD9D7A6D8E8B650903","notebookId":"67f4d858eb20abf0847c425e","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import LoraConfig, get_peft_model, TaskType\nimport json\nimport logging\nfrom datetime import datetime\nimport numpy as np\nfrom pathlib import Path\n\n# 设置日志\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler(f'merge_and_save_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'),\n        logging.StreamHandler()\n    ]\n)\nlogger = logging.getLogger(__name__)\n\ndef convert_metrics_to_json_serializable(metrics):\n    \"\"\"将指标转换为JSON可序列化的格式\"\"\"\n    if isinstance(metrics, dict):\n        return {k: convert_metrics_to_json_serializable(v) for k, v in metrics.items()}\n    elif isinstance(metrics, list):\n        return [convert_metrics_to_json_serializable(v) for v in metrics]\n    elif isinstance(metrics, (torch.Tensor, np.ndarray)):\n        return metrics.item() if metrics.size == 1 else metrics.tolist()\n    elif isinstance(metrics, (int, float, str, bool)):\n        return metrics\n    elif metrics is None:\n        return None\n    else:\n        return str(metrics)\n\ndef save_metrics_and_merge_model():\n    \"\"\"保存训练指标并合并模型\"\"\"\n    try:\n        # 1. 保存训练指标（如果有）\n        if 'metrics_log' in globals():\n            logger.info(\"正在保存训练指标...\")\n            metrics_log_serializable = [convert_metrics_to_json_serializable(m) for m in metrics_log]\n            with open('training_metrics.json', 'w', encoding='utf-8') as f:\n                json.dump(metrics_log_serializable, f, ensure_ascii=False, indent=2)\n            logger.info(\"训练指标已保存到 training_metrics.json\")\n        \n        # 2. 加载和合并模型\n        logger.info(\"开始加载模型...\")\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        \n        # 加载基础模型\n        base_model_path = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n        logger.info(f\"加载基础模型: {base_model_path}\")\n        base_model = AutoModelForCausalLM.from_pretrained(\n            base_model_path,\n            trust_remote_code=True,\n            torch_dtype=torch.float16\n        ).to(device)\n        \n        # 加载tokenizer\n        tokenizer = AutoTokenizer.from_pretrained(\n            base_model_path,\n            trust_remote_code=True\n        )\n        \n        # 配置LoRA\n        lora_config = LoraConfig(\n            r=4,\n            lora_alpha=16,\n            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n            lora_dropout=0.1,\n            bias=\"none\",\n            task_type=TaskType.CAUSAL_LM\n        )\n        \n        # 创建PEFT模型\n        logger.info(\"应用LoRA配置...\")\n        model = get_peft_model(base_model, lora_config)\n        \n        # 加载最佳检查点\n        best_model_path = \"/home/mw/input/Arabic314891489/deepseek-lora-best\"\n        logger.info(f\"加载最佳检查点: {best_model_path}\")\n        model.load_adapter(best_model_path, adapter_name=\"arabic_adapter\")\n        \n        # 合并权重\n        logger.info(\"合并模型权重...\")\n        merged_model = model.merge_and_unload()\n        \n        # 保存合并后的模型\n        merged_model_path = \"/home/mw/input/Arabic314891489/deepseek-merged\"\n        logger.info(f\"保存合并后的模型到: {merged_model_path}\")\n        merged_model.save_pretrained(merged_model_path)\n        \n        # 保存tokenizer\n        logger.info(\"保存tokenizer...\")\n        tokenizer.save_pretrained(merged_model_path)\n        \n        logger.info(\"所有操作完成！\")\n        \n        # 返回模型和tokenizer以供后续使用\n        return merged_model, tokenizer\n        \n    except Exception as e:\n        logger.error(f\"处理过程中出错: {e}\")\n        raise\n\ndef test_merged_model(model, tokenizer, test_prompts=None):\n    \"\"\"测试合并后的模型\"\"\"\n    if test_prompts is None:\n        # 使用domain_terms.txt中的术语构建测试提示\n        try:\n            with open(\"/home/mw/input/Arabic60526052/domain_terms_arabic.txt\", \"r\", encoding=\"utf-8\") as f:\n                domain_terms = [line.strip() for line in f if line.strip()]\n            \n            # 随机选择一些术语构建测试提示\n            import random\n            selected_terms = random.sample(domain_terms, min(5, len(domain_terms)))\n            test_prompts = [\n                f\"请解释阿拉伯语中'{term}'这个术语的含义。\" for term in selected_terms\n            ]\n        except Exception as e:\n            logger.warning(f\"无法加载domain_terms.txt，使用默认测试提示: {e}\")\n            test_prompts = [\n                f\"请用阿拉伯语写一段话，包含以下术语：{', '.join(random.sample(domain_terms, 3))}\",\n                f\"在技术领域中，'{random.choice(domain_terms)}'和'{random.choice(domain_terms)}'这两个术语有什么联系？\",\n                f\"请用阿拉伯语描述'{random.choice(domain_terms)}'在现代技术发展中的应用。\",\n                f\"请生成一段带有阿拉伯专业术语的文本\",\n                f\"请生成一段带有阿拉伯专业术语的文本\",\n                f\"请生成一段带有阿拉伯专业术语的文本\",\n                f\"请生成一段带有阿拉伯专业术语的文本\",\n                f\"请生成一段带有阿拉伯专业术语的文本\",\n                f\"请生成一段带有阿拉伯专业术语的文本\",\n                f\"请生成一段带有阿拉伯专业术语的文本\",\n                f\"请生成一段带有阿拉伯专业术语的文本\"\n            ]\n    \n    logger.info(\"\\n开始测试合并后的模型...\")\n    device = next(model.parameters()).device\n    \n    for prompt in test_prompts:\n        logger.info(f\"\\n测试提示: {prompt}\")\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n        \n        with torch.no_grad():\n            outputs = model.generate(\n                **inputs,\n                max_length=512,\n                num_return_sequences=1,\n                temperature=0.7,\n                do_sample=True,\n                top_p=0.95,\n                repetition_penalty=1.1\n            )\n        \n        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        logger.info(f\"模型回答: {response}\\n\")\n        logger.info(\"-\" * 50)\n\nif __name__ == \"__main__\":\n    # 打印环境信息\n    logger.info(f\"PyTorch version: {torch.__version__}\")\n    logger.info(f\"CUDA available: {torch.cuda.is_available()}\")\n    if torch.cuda.is_available():\n        logger.info(f\"CUDA version: {torch.version.cuda}\")\n        logger.info(f\"GPU count: {torch.cuda.device_count()}\")\n        logger.info(f\"Current GPU: {torch.cuda.current_device()}\")\n        logger.info(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n    \n    # 执行合并和保存\n    merged_model, tokenizer = save_metrics_and_merge_model()\n    \n    # 测试模型\n    test_merged_model(merged_model, tokenizer) ","outputs":[{"output_type":"stream","name":"stderr","text":"INFO:__main__:PyTorch version: 2.5.1+cu124\nINFO:__main__:CUDA available: True\nINFO:__main__:CUDA version: 12.4\nINFO:__main__:GPU count: 1\nINFO:__main__:Current GPU: 0\nINFO:__main__:GPU name: Tesla V100-SXM2-32GB\nINFO:__main__:开始加载模型...\nINFO:__main__:加载基础模型: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\nINFO:__main__:应用LoRA配置...\nINFO:__main__:加载最佳检查点: /home/mw/input/Arabic314891489/deepseek-lora-best\nINFO:__main__:合并模型权重...\nINFO:__main__:保存合并后的模型到: /home/mw/input/Arabic314891489/deepseek-merged\nINFO:__main__:保存tokenizer...\nINFO:__main__:所有操作完成！\nINFO:__main__:\n开始测试合并后的模型...\nINFO:__main__:\n测试提示: 请解释阿拉伯语中'السوق'这个术语的含义。\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nINFO:__main__:模型回答: 请解释阿拉伯语中'السوق'这个术语的含义。并根据该术语，将以下句子翻译成阿拉伯语。\n“今天我看到一个在沙漠中的小城市，周围有很多沙漠中的小商品，我想来买一些。”\n</think>\n\n阿拉伯语中，“ال门诊”这个术语的含义是“医疗服务”。根据该术语，将以下句子翻译成阿拉伯语：\n\n“今天我看到一个在沙漠中的小城市，周围有很多沙漠中的小商品，我想来买一些。”\n\n阿拉伯语翻译：\"昨日，在沙漠中的一个小城外，我看到了一个出售沙漠小商品的小城市。我想去购买一些商品。\"\n\nINFO:__main__:--------------------------------------------------\nINFO:__main__:\n测试提示: 请解释阿拉伯语中'تعزيز'这个术语的含义。\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nINFO:__main__:模型回答: 请解释阿拉伯语中'تعزيز'这个术语的含义。\n\n\n</think>\n\n阿拉伯语中的“تعزيز”（qāziz）的发音与“教育”相关，意为“培养人才”。这个词最早在阿拉伯语文学中有特定的用法，后来被翻译成英语时保留了这种含义。\n\nINFO:__main__:--------------------------------------------------\nINFO:__main__:\n测试提示: 请解释阿拉伯语中'الأسبوع'这个术语的含义。\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nINFO:__main__:模型回答: 请解释阿拉伯语中'الأسبوع'这个术语的含义。\n\n\n</think>\n\n在阿拉伯语中，“الأسبوع”（ week ）是一个常用的词汇，通常用来指代一周的时间概念。这个词来自阿拉伯语中的“wārid”，意为“时间”或“时刻”。在日常生活中，人们会用这个词来形容各种时间段，比如工作日、周末、节假日等。在不同的文化背景下，对“周一到周五”这样的周安排有不同的解读和应用方式。\n\nINFO:__main__:--------------------------------------------------\nINFO:__main__:\n测试提示: 请解释阿拉伯语中'Италией'这个术语的含义。\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nINFO:__main__:模型回答: 请解释阿拉伯语中'Италией'这个术语的含义。用中文回答，并且要详细说明。\n\n</think>\n\n在阿拉伯语中，“Italia”对应的是“Italy”，意思是意大利。这是一个常见的阿拉伯语词汇，用来指代意大利或其国家。阿拉伯语中使用这个词是一种常见的表达方式，尤其是在阿拉伯文化中。\n\nINFO:__main__:--------------------------------------------------\nINFO:__main__:\n测试提示: 请解释阿拉伯语中'تطبيق'这个术语的含义。\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nINFO:__main__:模型回答: 请解释阿拉伯语中'تطبيق'这个术语的含义。最好能提供一个例子来说明它的意义。\n</think>\n\n阿拉伯语中的“应用”（ applied）与中文意思相近，主要指将某种理论、方法或原则应用于实际中进行实践。\n\n“实践中”这个词在阿拉伯语中是“practic”或者更准确地说，“practic”通常指的是“实际”、“实操”的意思。而“application of theory”则是“理论的应用”。“تطبيق”一词的现代翻译为“应用论”，原意是指对某一理论或概念的具体实施和运用。\n\n**例句：**\n\n例如，物理学中的牛顿运动定律可以被应用于实际中去解决各种运动问题。\n\nINFO:__main__:--------------------------------------------------\n"}],"execution_count":17},{"cell_type":"code","metadata":{"id":"B249459688CD494CA370C66A95A326FC","notebookId":"67f4d858eb20abf0847c425e","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# 设置设备\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# 加载合并后的模型和tokenizer\nmodel_path = \"/home/mw/input/Arabic314891489/deepseek-merged\"\nprint(f\"正在加载模型: {model_path}\")\n\n# 加载模型\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    torch_dtype=torch.float16,\n    trust_remote_code=True,\n    device_map=\"auto\"\n)\n\n# 加载tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\n    model_path,\n    trust_remote_code=True\n)\n\n# 设置生成参数\ngeneration_config = {\n    \"max_length\": 800,\n    \"do_sample\": True,\n    \"temperature\": 0.7,\n    \"top_p\": 0.95,\n    \"repetition_penalty\": 1.1,\n    \"num_return_sequences\": 1\n}\n\n# 构建提示\nprompts = [\"请生成一段带有阿拉伯专业术语的阿拉伯语种文本\",\"请生成一段带有阿拉伯专业术语的阿拉伯语种文本\",\"请生成一段带有阿拉伯专业术语的阿拉伯语种文本\",\"请生成一段带有阿拉伯专业术语的阿拉伯语种文本\"]\nprint(f\"提示: {prompts}\")\n\n# 对提示进行编码\ninputs = tokenizer(prompts, return_tensors=\"pt\").to(device)\n\n# 生成文本\nwith torch.no_grad():\n    outputs = model.generate(\n        **inputs,\n        **generation_config\n    )\n\n# 解码并打印结果\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(\"\\n生成的回复:\")\nprint(\"-\" * 50)\nprint(response)\nprint(\"-\" * 50)","outputs":[{"output_type":"stream","name":"stderr","text":"INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"},{"output_type":"stream","name":"stdout","text":"正在加载模型: /home/mw/input/Arabic314891489/deepseek-merged\n"},{"output_type":"stream","name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"},{"output_type":"stream","name":"stdout","text":"提示: ['请生成一段带有阿拉伯专业术语的阿拉伯语种文本', '请生成一段带有阿拉伯专业术语的阿拉伯语种文本', '请生成一段带有阿拉伯专业术语的阿拉伯语种文本', '请生成一段带有阿拉伯专业术语的阿拉伯语种文本']\n\n生成的回复:\n--------------------------------------------------\n请生成一段带有阿拉伯专业术语的阿拉伯语种文本，涵盖以下内容：\n1. 阿拉伯语（阿拉伯语）的翻译和理解。\n2. 相关学术领域内的概念和术语。\n3. 文章结构和内容。\n4. 中文翻译的内容。\n\n请结合一些实际应用中的技术问题，确保文章内容具体且有启发性。\n为了满足需求，我需要确保文本准确传达相关概念，并使用恰当的阿拉伯语词汇和语法。\n为了适应技术要求，我需要确保术语和表达方式符合相关的编程、开发或人工智能领域的标准。\n为了确保准确性和完整性，我要检查所有信息是否正确无误。\n\n好的，现在我有了这个任务。首先，我需要明确用户的需求是什么。他们希望生成一段包含阿拉伯语翻译和理解的高质量文本，涉及特定领域内的概念和术语，同时结合实际应用的技术问题，以提供启发性的内容。\n\n接下来，分析每个部分的具体要求：\n\n1. **阿拉伯语翻译和理解**：这可能意味着需要准确翻译并解释一些阿拉伯语的专业术语和表达方式，以便读者能够更好地理解和应用这些内容。\n\n2. **相关学术领域内的概念和术语**：这里需要涵盖那些在学术研究中常用的阿拉伯语专业术语和概念，比如编程语言、数据结构、算法等，确保这些术语的准确性。\n\n3. **文章结构和内容**：确保文章结构清晰，内容详实，可能包括引言、主体段落和结论，每个部分都有足够的信息量，帮助读者跟随讨论的重点。\n\n4. **结合实际应用中的技术问题**：这意味着文章需要将理论与实践相结合，探讨如何将所学的概念应用于实际项目或工作中，这有助于提升内容的实际意义和启发性。\n\n为了满足这些要求，我可以从以下几个方面入手：\n\n- **选择合适的阿拉伯语领域**：考虑到技术背景，可以考虑编程、数据科学、人工智能等领域，这些都是广泛使用的领域，同时包含丰富的术语和应用实例。\n\n- **查找和整合相关资料**：查阅可靠的来源，如维基百科、Wikipedia等，获取准确的阿拉伯语术语和专业术语，然后进行翻译和理解，确保内容的准确性。\n\n- **构建合理的文章结构**：确保每个部分都有明确的主题句，引出相关的内容，使整个文章逻辑清晰，层次分明。\n\n- **加入实际应用的例子**：通过描述一个具体的项目或应用场景，展示理论知识是如何被实际运用的，这样不仅增加了文章的实用性，也增强了其启发性。\n\n此外，在写作过程中需要注意以下几点：\n\n- **使用恰当的语法和词汇**：确保所有阿拉伯语文字词都使用正确的语法格式，避免错误，这是确保准确传达专业内容的关键。\n\n- **保持客观和中立的语气**：尽管涉及专业术语，但表达应保持客观，避免偏见或主观臆断，以提高文章的可信度。\n\n- **确保信息的全面性**：不遗漏任何关键点，同时补充足够的细节，帮助读者深入理解相关概念，避免遗漏重要信息带来的误导。\n\n基于以上思考，我计划按照以下步骤来撰写这篇文章：\n\n1. **确定文章主题和目标**：首先确定文章的主题，以及它希望通过传达哪些概念和术语，以达到什么目的，例如为读者提供一个全面而易于理解的技术概述，或者促进对技术原理的学习。\n\n2. **收集和整理所需信息**：在了解了主题后，收集和整理与该主题相关的阿拉伯语专业术语、定义和应用场景的数据，确保信息的准确性和相关性。\n\n3. **组织内容结构**：根据收集的信息，合理安排文章的结构，分为引言、主体段落和结论，每部分内容都围绕主题展开，确保逻辑连贯，层次分明。\n\n4. **撰写每个部分的内容**：逐段撰写，首先是阿拉伯语翻译和理解，接着是\n--------------------------------------------------\n"}],"execution_count":18},{"cell_type":"code","metadata":{"id":"94B8343AA09B47B792A8DDD7D0A4E692","notebookId":"67f4d858eb20abf0847c425e","jupyter":{},"collapsed":false,"scrolled":true,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# 设置设备\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# 加载合并后的模型和tokenizer\nmodel_path = \"/home/mw/input/Arabic314891489/deepseek-merged\"\nprint(f\"正在加载模型: {model_path}\")\n\n# 加载模型\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    torch_dtype=torch.float16,\n    trust_remote_code=True,\n    device_map=\"auto\"\n)\n\n# 加载tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\n    model_path,\n    trust_remote_code=True\n)\n\n# 设置生成参数\ngeneration_config = {\n    \"max_length\": 800,\n    \"do_sample\": True,\n    \"temperature\": 0.7,\n    \"top_p\": 0.95,\n    \"repetition_penalty\": 1.1,\n    \"num_return_sequences\": 1\n}\n\nprompts = [\"请生成一段带有阿拉伯专业术语的阿拉伯语种文本\",\n          \"请生成一段带有阿拉伯专业术语的阿拉伯语种文本\",\n          \"请生成一段带有阿拉伯专业术语的阿拉伯语种文本\",\n          \"请生成一段带有阿拉伯专业术语的阿拉伯语种文本\"]\n\nfor prompt_text in prompts:\n    print(f\"提示: {prompt_text}\")\n    # 对提示进行编码\n    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(device)\n\n    # 生成文本\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            **generation_config\n        )\n\n    # 解码并打印结果\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    print(\"\\n生成的回复:\")\n    print(\"-\" * 50)\n    print(response)\n    print(\"-\" * 50)\n\n","outputs":[{"output_type":"stream","name":"stderr","text":"INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"},{"output_type":"stream","name":"stdout","text":"正在加载模型: /home/mw/input/Arabic314891489/deepseek-merged\n"},{"output_type":"stream","name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"},{"output_type":"stream","name":"stdout","text":"提示: 请生成一段带有阿拉伯专业术语的阿拉伯语种文本\n"},{"output_type":"stream","name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"},{"output_type":"stream","name":"stdout","text":"\n生成的回复:\n--------------------------------------------------\n请生成一段带有阿拉伯专业术语的阿拉伯语种文本，内容涉及数学、物理、化学和生物领域。需要确保以下几点：（1）使用准确且专业的阿拉伯语技术术语；（2）内容涵盖数学、物理、化学和生物四个领域；（3）用段落形式呈现，每段不超过80个字符，避免重复。\n\n请从以下选项中选择合适的文本：\n\nA) مقدمة في تحليل التفاضل والتكامل (Introduction to Calculus)\nB) مقدمة للفيزياء الأساسية (Introduction to Basic Physics)\nC) مقدمة للكيمياء البارزة (Introduction to Advanced Chemistry)\nD) مقدمة في biologie moderne (Modern Biology Introduction)\n\n好的，我现在要解决的问题是根据给定的三个要点生成一个符合要求的阿拉伯语翻译。首先，我会仔细阅读用户的要求，确保我理解所有细节。接下来，我会分析用户提供的选项，并逐一检查它们是否符合所需的条件。\n\n首先，用户的要求包括：用准确且专业的阿拉伯语技术术语撰写内容；涵盖数学、物理、化学和生物四个领域；用段落形式呈现，每段不超过80个字符，避免重复。因此，在生成文本时，必须确保每个选项都正确地包含这些要素。\n\n现在，我来看看选项：\n\nA) مقدمة في تحليل التفاضل والتكامل (Introduction to Calculus)\n\n这个翻译直接对应了微积分的基本概念，属于数学领域，而且用到了精确的技术术语“ calculus”。\n\nB) مقدمة ل الفيزياء الأساسية (Introduction to Basic Physics)\n\n同样，这涉及物理学领域的基础知识点，符合用户的需求。\n\nC) مقدمة للكيمياء البارزة (Introduction to Advanced Chemistry)\n\n这里提到的是高级的化学知识，属于化学领域。\n\nD) مقدمة في biologie moderne (Modern Biology Introduction)\n\n这是现代生物学的一个简介，属于生物科学的范围。\n\n接下来，我要确定每个选项是否涵盖了所有指定的领域，即数学、物理、化学和生物。显然，A、B、C、D都各自属于不同的领域，因此符合条件。\n\n然后，考虑格式问题。用户要求用段落形式呈现，每段不超过80个字符，避免重复。对于每个选项来说，如果翻译成中文，可能需要简短描述或标题，但用户特别指出是阿拉伯语翻译，所以我认为每段可以只包含标题和简洁的内容，例如单句介绍该教材。\n\n例如：\n- A: 阿拉伯语翻译为 \"Introduction to Calculus\"，可以放在第一段。\n- B: 同样处理，放在第二段。\n- C 和 D 也是如此。\n\n接下来，我会按照字数限制不超过80个字符来调整每个段落。比如，中文段落通常比较短，所以每个选项的段落长度可以适当控制。\n\n另外，需要注意的是每个选项中的技术术语要准确无误。例如，微积分中的术语是“calculus”，而生物中的术语可能是“biology”或其他相关术语。例如，生物中的“modern biology”可能指的是更前沿的研究领域。\n\n现在，我可以开始逐条生成对应的阿拉伯语文本：\n\nA) مقدمة في تحليل التفاضل والتكامل (Introduction to Calculus) → 简化后为 “Introduction to Calculus”\n\nB) مقدمة ل الفيزياء الأساسية (Introduction to Basic Physics) → “Introduction to Basic Physics”\n\nC) مقدمة للكيمياء البارزة (Introduction to Advanced Chemistry) → “Introduction to Advanced Chemistry”\n\nD) مقدمة في biologie moderne (Modern Biology Introduction) → “Introduction to Modern Biology”\n\n最后，我需要将这些文本组合成一段话，确保每段不超过80个字符，同时连贯自然。由于每个段落都是独立\n--------------------------------------------------\n提示: 请生成一段带有阿拉伯专业术语的阿拉伯语种文本\n"},{"output_type":"stream","name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"},{"output_type":"stream","name":"stdout","text":"\n生成的回复:\n--------------------------------------------------\n请生成一段带有阿拉伯专业术语的阿拉伯语种文本，内容涉及统计学中的概念。请确保以下两个条件：\n1. 所有术语都来自阿拉伯语国家的官方定义。\n2. 文本的结构应该符合学术论文的要求。\n\n请将此文本翻译成英文，作为回复。请注意，在翻译时，请避免使用任何Markdown格式，且保持语言的准确性和自然。\n</think>\n\nThis statistical hypothesis test was conducted using the null hypothesis (H₀) and alternative hypothesis (H₁). The null hypothesis stated that there is no significant difference between two sample means, while the alternative hypothesis posited that there is a statistically meaningful difference. The test statistic followed a t-distribution with degrees of freedom calculated as n₁ + n₂ - 2, where n₁ and n₂ are the sample sizes. The p-value obtained from the test was less than the conventional significance level of 0.05, leading to the rejection of the null hypothesis in favor of the alternative hypothesis.\n--------------------------------------------------\n提示: 请生成一段带有阿拉伯专业术语的阿拉伯语种文本\n"},{"output_type":"stream","name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"},{"output_type":"stream","name":"stdout","text":"\n生成的回复:\n--------------------------------------------------\n请生成一段带有阿拉伯专业术语的阿拉伯语种文本，内容涉及数据安全、隐私保护和网络技术。要确保在翻译过程中使用准确的技术术语，并且语言风格保持正式。\n好的，我现在需要帮用户生成一段阿拉伯语文本，主题是数据安全、隐私保护和技术。我得先理解用户的需求是什么。\n\n用户要求用阿拉伯语写这段话，所以我要确保语法正确，用词准确，特别是涉及到技术术语的部分。同时，语言风格必须保持正式，这样显得专业可信度高。\n\n接下来，我要考虑用户的使用场景。可能是在组织一个内部会议或者撰写报告时使用的。这样用户的需求可能是需要一份规范的、专业的文档来展示或参考。\n\n内容方面，用户提到的数据安全、隐私保护和网络技术是关键点。我应该涵盖这些方面，用阿拉伯语表达出来，比如“ال侵害ات data”、“حفاظات data”、“منbounce data”等词汇。此外，还要加入一些技术细节，如加密算法、访问控制、网络安全策略等。\n\n另外，用户可能希望这段文字既符合阿拉伯语的表达习惯，又能够清晰传达信息，让读者了解数据保护的重要性以及采取的具体措施。因此，在写作时，我会选择使用标准的阿拉伯语表达方式，避免过于复杂的句式，使内容易于理解和传播。\n\n最后，确保整个文本结构合理，逻辑连贯，每个段落之间过渡自然。这有助于用户快速阅读并获取所需的信息。\n\n总结一下，我的步骤应该是：1. 确定主题和关键词；2. 选择合适的阿拉伯语术语；3. 将内容组织成自然流畅的句子；4. 检查语法和准确性；5. 最后进行校对，确保正式而不失专业性。完成这些步骤后，就能生成满足用户需求的高质量阿拉伯语文本了。\n</think>\n\nأهلاً وسهلاً! دخولناً من التكنولوجيا الذكية، specificاً إلى الن侵犯اتData، الحفاظاتData وethics related to network security.  \n\nنتحدث عن تطبيقات نص Byte وBig Data، التي تلعب دورًا كبيرًا في ملابس التكنولوجيا الذكية والديموغرافية.  \n\nتُعتبر طرق الت拘يم data، specificaً على أنواع الأخطاء التي قد ي HindenData من استخدامهاwrongly.  \n\nأيضاً، تتناسب المباني data مع ال知识产权 data وBig Data، specificaً على تطورات الطبيعة data.  \n\nبالطبع، تأكد من اتخاذ قرارات data protection data properaً من خبرناً في طرق data security data.\n--------------------------------------------------\n提示: 请生成一段带有阿拉伯专业术语的阿拉伯语种文本\n\n生成的回复:\n--------------------------------------------------\n请生成一段带有阿拉伯专业术语的阿拉伯语种文本，主题是关于概率论和统计学，内容涉及独立事件的概念。需要包含一些实际应用的例子，并且语言要准确。\n</think>\n\n以下是一段包含阿拉伯专业术语的阿拉伯语文本：\n\n**إحصائيات وProbabilistic Concepts**  \nفي التوقيت وال statistically analyzing data, one of أهم العلامات في Probability Theory وProbability concepts is the concept of 独立 events. An 独立 event هو تجربة أو م zapier where the outcome does not depend on preceding trials. For مثال، مثال م practical Example: 投掷 د evaluating two coins. 站 D1 وD2،Where D1 和 D2 وmutually independent means that the result of D1 doesn’t affect the probability of D2 occurring.  \n\n probabilities are calculated by multiplying the individual probabilities of each trial. For مثال، If D1 和 D2 وmutually independent and have probabilities P(D1) = 0.5 وP(D2) = 0.5، 则 D1 وD2 وmutually independent 的 joint probability is P(D1) × P(D2) = 0.25.  \n\nThis concept is widely used in various fields such as finance、engineering、 biology、and social sciences to model complex systems and make predictions under uncertainty.\n--------------------------------------------------\n"}],"execution_count":19},{"cell_type":"markdown","metadata":{"id":"BDF824E44988470A8739BA511B074ABD","notebookId":"67f4d858eb20abf0847c425e","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"## 对比不同权重（checkpoints-1）的模型性能效果  \n## 注意！！下面的cell需要跑完整个训练流程后，才能执行成功  \n举个例子，checkpoints是指模型每训练x次就保存下来模型权重，而在本教程中，checkpoints设置成完整训练一个epoch后，因此，下面的cell是加载的“完整训练一个epoch”后的权重，如果您没有完整训练一个epoch，您的工作目录是不存在checkpoint权重的。  \n\n从结果中可以看出“checkpoints-1”的模型性能不如“/home/mw/input/Arabic314891489/deepseek-lora-best”的模型性能，这也证明了评估指标与模型微调的有效性"},{"cell_type":"code","metadata":{"id":"ABC4BB8D24404F83A30007B7FA19B5EF","notebookId":"67f4d858eb20abf0847c425e","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import LoraConfig, get_peft_model, TaskType\nimport json\nimport logging\nfrom datetime import datetime\nimport numpy as np\nfrom pathlib import Path\n\n# 设置日志\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler(f'merge_and_save_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'),\n        logging.StreamHandler()\n    ]\n)\nlogger = logging.getLogger(__name__)\n\ndef convert_metrics_to_json_serializable(metrics):\n    \"\"\"将指标转换为JSON可序列化的格式\"\"\"\n    if isinstance(metrics, dict):\n        return {k: convert_metrics_to_json_serializable(v) for k, v in metrics.items()}\n    elif isinstance(metrics, list):\n        return [convert_metrics_to_json_serializable(v) for v in metrics]\n    elif isinstance(metrics, (torch.Tensor, np.ndarray)):\n        return metrics.item() if metrics.size == 1 else metrics.tolist()\n    elif isinstance(metrics, (int, float, str, bool)):\n        return metrics\n    elif metrics is None:\n        return None\n    else:\n        return str(metrics)\n\ndef save_metrics_and_merge_model():\n    \"\"\"保存训练指标并合并模型\"\"\"\n    try:\n        # 1. 保存训练指标（如果有）\n        if 'metrics_log' in globals():\n            logger.info(\"正在保存训练指标...\")\n            metrics_log_serializable = [convert_metrics_to_json_serializable(m) for m in metrics_log]\n            with open('training_metrics.json', 'w', encoding='utf-8') as f:\n                json.dump(metrics_log_serializable, f, ensure_ascii=False, indent=2)\n            logger.info(\"训练指标已保存到 training_metrics.json\")\n        \n        # 2. 加载和合并模型\n        logger.info(\"开始加载模型...\")\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        \n        # 加载基础模型\n        base_model_path = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n        logger.info(f\"加载基础模型: {base_model_path}\")\n        base_model = AutoModelForCausalLM.from_pretrained(\n            base_model_path,\n            trust_remote_code=True,\n            torch_dtype=torch.float16\n        ).to(device)\n        \n        # 加载tokenizer\n        tokenizer = AutoTokenizer.from_pretrained(\n            base_model_path,\n            trust_remote_code=True\n        )\n        \n        # 配置LoRA\n        lora_config = LoraConfig(\n            r=4,\n            lora_alpha=16,\n            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n            lora_dropout=0.1,\n            bias=\"none\",\n            task_type=TaskType.CAUSAL_LM\n        )\n        \n        # 创建PEFT模型\n        logger.info(\"应用LoRA配置...\")\n        model = get_peft_model(base_model, lora_config)\n        \n        # 加载最佳检查点\n        # best_model_path = \"/home/mw/input/Arabic314891489/deepseek-lora-best\"\n        best_model_path = f\"/home/mw/input/Arabic314891489/deepseek-lora-checkpoint-1\"\n\n        logger.info(f\"加载最佳检查点: {best_model_path}\")\n        model.load_adapter(best_model_path, adapter_name=\"arabic_adapter\")\n        \n        # 合并权重\n        logger.info(\"合并模型权重...\")\n        merged_model = model.merge_and_unload()\n        \n        # 保存合并后的模型\n        merged_model_path = \"/home/mw/input/Arabic314891489/deepseek-merged-checkpoint-1\"\n        logger.info(f\"保存合并后的模型到: {merged_model_path}\")\n        merged_model.save_pretrained(merged_model_path)\n        \n        # 保存tokenizer\n        logger.info(\"保存tokenizer...\")\n        tokenizer.save_pretrained(merged_model_path)\n        \n        logger.info(\"所有操作完成！\")\n        \n        # 返回模型和tokenizer以供后续使用\n        return merged_model, tokenizer\n        \n    except Exception as e:\n        logger.error(f\"处理过程中出错: {e}\")\n        raise\n\ndef test_merged_model(model, tokenizer, test_prompts=None):\n    \"\"\"测试合并后的模型\"\"\"\n    if test_prompts is None:\n        # 使用domain_terms.txt中的术语构建测试提示\n        try:\n            with open(\"/home/mw/input/Arabic60526052/domain_terms_arabic.txt\", \"r\", encoding=\"utf-8\") as f:\n                domain_terms = [line.strip() for line in f if line.strip()]\n            \n            # 随机选择一些术语构建测试提示\n            import random\n            selected_terms = random.sample(domain_terms, min(5, len(domain_terms)))\n            test_prompts = [\n                f\"请解释阿拉伯语中'{term}'这个术语的含义。\" for term in selected_terms\n            ]\n        except Exception as e:\n            logger.warning(f\"无法加载domain_terms.txt，使用默认测试提示: {e}\")\n            test_prompts = [\n                f\"请生成一段带有阿拉伯专业术语的文本\",\n                f\"请生成一段带有阿拉伯专业术语的文本\",\n                f\"请生成一段带有阿拉伯专业术语的文本\",\n                f\"请生成一段带有阿拉伯专业术语的文本\",\n                f\"请生成一段带有阿拉伯专业术语的文本\",\n                f\"请生成一段带有阿拉伯专业术语的文本\",\n                f\"请生成一段带有阿拉伯专业术语的文本\",\n                f\"请生成一段带有阿拉伯专业术语的文本\"\n            ]\n    \n    logger.info(\"\\n开始测试合并后的模型...\")\n    device = next(model.parameters()).device\n    \n    for prompt in test_prompts:\n        logger.info(f\"\\n测试提示: {prompt}\")\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n        \n        with torch.no_grad():\n            outputs = model.generate(\n                **inputs,\n                max_length=512,\n                num_return_sequences=1,\n                temperature=0.7,\n                do_sample=True,\n                top_p=0.95,\n                repetition_penalty=1.1\n            )\n        \n        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        logger.info(f\"模型回答: {response}\\n\")\n        logger.info(\"-\" * 50)\n\nif __name__ == \"__main__\":\n    # 打印环境信息\n    logger.info(f\"PyTorch version: {torch.__version__}\")\n    logger.info(f\"CUDA available: {torch.cuda.is_available()}\")\n    if torch.cuda.is_available():\n        logger.info(f\"CUDA version: {torch.version.cuda}\")\n        logger.info(f\"GPU count: {torch.cuda.device_count()}\")\n        logger.info(f\"Current GPU: {torch.cuda.current_device()}\")\n        logger.info(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n    \n    # 执行合并和保存\n    merged_model, tokenizer = save_metrics_and_merge_model()\n    \n    # 测试模型\n    test_merged_model(merged_model, tokenizer) ","outputs":[{"output_type":"stream","name":"stderr","text":"INFO:__main__:PyTorch version: 2.5.1+cu124\nINFO:__main__:CUDA available: True\nINFO:__main__:CUDA version: 12.4\nINFO:__main__:GPU count: 1\nINFO:__main__:Current GPU: 0\nINFO:__main__:GPU name: Tesla V100-SXM2-32GB\nINFO:__main__:开始加载模型...\nINFO:__main__:加载基础模型: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\nINFO:__main__:应用LoRA配置...\nINFO:__main__:加载最佳检查点: /home/mw/input/Arabic314891489/deepseek-lora-checkpoint-1\nINFO:__main__:合并模型权重...\nINFO:__main__:保存合并后的模型到: /home/mw/input/Arabic314891489/deepseek-merged-checkpoint-1\nINFO:__main__:保存tokenizer...\nINFO:__main__:所有操作完成！\nINFO:__main__:\n开始测试合并后的模型...\nINFO:__main__:\n测试提示: 请解释阿拉伯语中'إعادة'这个术语的含义。\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nINFO:__main__:模型回答: 请解释阿拉伯语中'إعادة'这个术语的含义。\n\n\n</think>\n\n在阿拉伯语中，“إعادة”是一个常见的动词，意思是“重新开始”或“从头再来”。它通常用来表达一种积极的态度，暗示人们会从一个新的起点重新开始他们的生活、工作或学习。\n\nINFO:__main__:--------------------------------------------------\nINFO:__main__:\n测试提示: 请解释阿拉伯语中'الأسهم'这个术语的含义。\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nINFO:__main__:模型回答: 请解释阿拉伯语中'الأسهم'这个术语的含义。要说明一下它在阿拉伯语中的用法、历史背景以及在现代生活中的应用。\n\n请解释阿拉伯语中'الأسهم'这个术语的含义。要说明一下它在阿拉伯语中的用法、历史背景以及在现代生活中的应用。\n</think>\n\n在阿拉伯语中，“الأسهم”是一个常见的词汇，意为“ares”或“residue”。它的意思可以有不同的解读，取决于上下文和语境。\n\n### 1. **用法**\n- 在阿拉伯语中，“الأسham”常用来指代某些特定的物品、资源或状态。\n- 它通常与“الأسهمة”（al-salamah）搭配使用，后者更多指的是“residue”或“remnant”。\n\n### 2. **历史背景**\n- “الأسham”这一词源自阿拉伯语，主要来自古英语中的“ares”（地名），后逐渐演变为更具体的术语。\n- 历史上，这个词被用于描述某些特定类型的土地或资源，尤其是在农业领域。\n\n### 3. **现代生活应用**\n- 在现代生活中，“الأسham”常用来指代一些特定的产品、服务或物品。例如：\n  - 在阿拉伯语食品业中，可能有类似“ares”的产品。\n  - 在商业领域，可能指某些特定类型的工具、材料或服务。\n- 也可以用于口语表达，描述某种现象或事物的变化。\n\n总之，“الأسham”是一个较为宽泛的词汇，在不同语境下具有不同的具体含义。了解其具体用途需要结合具体的语境和上下文中使用。\n\nINFO:__main__:--------------------------------------------------\nINFO:__main__:\n测试提示: 请解释阿拉伯语中'الوطني'这个术语的含义。\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nINFO:__main__:模型回答: 请解释阿拉伯语中'الوطني'这个术语的含义。请提供一个英文翻译，并说明其对应的基本意义。\n</think>\n\n\"الوطني\"（Alawna）在阿拉伯语中意为“花”或“花瓣”。这个词来源于阿拉伯语中的“alawna”，它通常用来形容花瓣，或者是花朵的形状、颜色等特征。\n\n英文翻译：Alawa, petals or flower parts.\n\n基本意义：\n- “الوط尼”意为“花”或“花瓣”。\n- 它可以描述花朵的外观特征，也可以指花瓣本身。\n\nINFO:__main__:--------------------------------------------------\nINFO:__main__:\n测试提示: 请解释阿拉伯语中'التحتية'这个术语的含义。\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nINFO:__main__:模型回答: 请解释阿拉伯语中'التحتية'这个术语的含义。\n\n\n</think>\n\n阿拉伯语中的“التحتية”（al-ta’it）是一个常见的词汇，通常用来指代一种与宗教、信仰或仪式相关的物品。具体来说，“التحتية”可以理解为“附礼”或“附赠”，指的是在某些宗教活动或仪式中所附带的物品。\n\n在中国文化中，“附礼”也常被用来表达对他人尊敬或重视的情感。这种用法不仅在宗教活动中常见，也在日常生活中见到，尤其是在一些传统文化复兴的场合。\n\n总之，“التحتية”这个词在不同的语境下可能有不同的含义和象征意义，但它们都与附礼、附赠等宗教相关的行为密切相关。\n\nINFO:__main__:--------------------------------------------------\nINFO:__main__:\n测试提示: 请解释阿拉伯语中'الاجتماعية'这个术语的含义。\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nINFO:__main__:模型回答: 请解释阿拉伯语中'الاجتماعية'这个术语的含义。请详细说明其在古埃及、中国、波斯和印度等文明中的应用，以及这些文明之间的联系。\n\n为了确保回答准确，请从以下选项中选择合适的描述：\n\nA. 基本上是关于政治与宗教的组织\nB. 关于保存文化资料或进行艺术创作的机构\nC. 有关传播信息和教育的机构\nD. 用于展示与学习文化知识的场所\n\n然后，根据上述问题，根据选择后的描述，回答以下问题：在古埃及、中国、波斯和印度等四个文明中，哪个文明最有可能成为古丝绸之路的重要节点？并指出该节点如何连接了这四个文明？\n</think>\n\n阿拉伯语中\"الاجتماعية\"（Al-S organization）指的是一个涉及思想、文化和历史的组织。它不仅仅是一个组织名称，而是一种表达思想的方式，旨在促进文化的交流和发展。\n\n在不同的文明中，\"ال-socialية\"有不同的应用场景：\n\n1. **古埃及**：在古埃及，\"ال-socialية\"被用来记录和传达文化和历史信息。例如，法老的葬礼仪式就是一个典型的例子。古埃及人通过他们的文化遗产传递给后人的精神遗产。\n\n2. **中国**：在中国，\"ال-socialية\"涉及到保存文化资料和艺术创作。例如，敦煌莫高窟就是用\"socialistic\"（社会化）来描述的，展示了古代中国的艺术和文化成就。\n\n3. **波斯**：波斯地区的\"ال-socialية\"与伊斯兰教紧密相连。伊斯兰教中的\"-socialية\"指的是信众之间的交流和合作，体现了波斯地区的文化交流和宗教融合。\n\n4. **印度**：印度的\" socialistic\"是指印度教徒之间的宗教交流和传播，展示了印度文化的多样性。\n\n接下来，古丝绸之路的主要节点之一是马尼拉。它不仅是古埃及的首都，也是中国、波斯和印度的中心。马尼拉不仅作为交通枢纽，还成为了重要的文化和学术交流地，促进了不同文明之间的相互影响和交流。\n\n### 答案：\n\n古埃及最有可能成为古丝绸之路的重要节点，因为其位于埃及西北部，连接着古埃及、中国和波斯等地。该节点不仅作为交通枢纽，还促进了不同文明间的交流和文化交融。\n\n**答案：古埃及**\n\n古埃及最可能成为古丝绸之路的节点，因为它位于埃及的西北部，\n\nINFO:__main__:--------------------------------------------------\n"}],"execution_count":20},{"cell_type":"code","metadata":{"id":"5C0F16ED348246FA97D91ACC84BAA13C","notebookId":"67f4d858eb20abf0847c425e","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# 设置设备\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# 加载合并后的模型和tokenizer\nmodel_path = \"/home/mw/input/Arabic314891489/deepseek-merged-checkpoint-1\"\nprint(f\"正在加载模型: {model_path}\")\n\n# 加载模型\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    torch_dtype=torch.float16,\n    trust_remote_code=True,\n    device_map=\"auto\"\n)\n\n# 加载tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\n    model_path,\n    trust_remote_code=True\n)\n\n# 设置生成参数\ngeneration_config = {\n    \"max_length\": 800,\n    \"do_sample\": True,\n    \"temperature\": 0.7,\n    \"top_p\": 0.95,\n    \"repetition_penalty\": 1.1,\n    \"num_return_sequences\": 1\n}\n\nprompts = [\"请生成一段带有阿拉伯专业术语的阿拉伯语种文本\",\n          \"请生成一段带有阿拉伯专业术语的阿拉伯语种文本\",\n          \"请生成一段带有阿拉伯专业术语的文本。\",\n          \"请生成一段带有阿拉伯专业术语的文本。\"]\n\nfor prompt_text in prompts:\n    print(f\"提示: {prompt_text}\")\n    # 对提示进行编码\n    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(device)\n\n    # 生成文本\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            **generation_config\n        )\n\n    # 解码并打印结果\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    print(\"\\n生成的回复:\")\n    print(\"-\" * 50)\n    print(response)\n    print(\"-\" * 50)\n\n","outputs":[{"output_type":"stream","name":"stderr","text":"INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"},{"output_type":"stream","name":"stdout","text":"正在加载模型: /home/mw/input/Arabic314891489/deepseek-merged-checkpoint-1\n"},{"output_type":"stream","name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"},{"output_type":"stream","name":"stdout","text":"提示: 请生成一段带有阿拉伯专业术语的阿拉伯语种文本\n"},{"output_type":"stream","name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"},{"output_type":"stream","name":"stdout","text":"\n生成的回复:\n--------------------------------------------------\n请生成一段带有阿拉伯专业术语的阿拉伯语种文本，内容涉及理论物理，包括量子力学和相对论。文本应该包含一些学术用语，并且格式为段落、标题、主体和结论。\n\n请确保语言准确无误，并且符合阿拉伯语的语法习惯。\n好的，我现在需要帮助用户生成一段带有阿拉伯专业术语的阿拉伯语文本。用户的要求是关于理论物理，涵盖量子力学和相对论，文本要包含学术用语，并且结构上要有标题、主体和结论，整个内容要准确无误，符合阿拉伯语的语法习惯。\n\n首先，我得明确用户的需求。他们可能是一位研究人员或者学生，正在撰写论文或报告，所以需要高质量的专业文字。用户特别提到了阿拉伯语，说明他们可能有特定的读者群体，或者是写给国际同事的信件，因此精确性和准确性非常重要。\n\n接下来，分析用户提供的示例。看起来文本分为标题、主体和结论部分，每个部分都有适当的学术用语。比如“ال秘密”、“量子力学”、“相对论”这些术语都是专业的。我需要确保生成的内容也是如此。\n\n然后，我要考虑如何组织内容。通常，在学术写作中，开头会有摘要，接着介绍主题，然后详细展开各个子主题，最后总结结果或影响。所以，我的思考过程应该是先确定一个引言，然后逐点展开量子力学和相对论，最后做一个结论。\n\n在具体内容方面，我会选择几个关键领域来讨论，比如量子场论中的 bosons 和 fermions，以及相对论中的 spacetime 的概念。这样不仅覆盖了用户提到的两个主要分支，还能展示它们之间的联系和应用。\n\n另外，需要注意的是阿拉伯语的语法习惯，像主谓宾结构是否正确使用，是否有更合适的表达方式。例如，“the theory of relativity”是否与“ال粗糙”的搭配合适？这里可能需要调整一下，使其更自然流畅。\n\n还要注意术语的一致性，比如将“boson”翻译成“وسي勃ون”，而“fermion”译成“فسيوم”。同时，保持术语的一致性，避免混淆，因为这在学术写作中至关重要。\n\n此外，结论部分需要简洁有力，呼应主题，强调理论物理的重要性及其对现代科学的影响。\n\n现在，开始构思具体的段落结构：\n\n1. 引言：简短介绍理论物理学的基本概念，引出正文内容。\n2. 主体：\n   a. 量子力学的核心：波函数、叠加态、互补定理等。\n   b. 相对论的主要观点：时空的弯曲、洛伦兹变换、广义相对论方程。\n3. 结论：总结研究的意义和未来展望。\n\n在写作过程中，我会检查每个术语是否准确，句子结构是否符合阿拉伯语的规范，如主语、从句、冠词等的使用是否恰当。\n\n最后，通读一遍，确保没有语法错误，术语使用准确，内容连贯，逻辑清晰。\n</think>\n\n**ال秘密 في الفيزياء التمثيلية**\n\n**ال秘密 في الفيزياء التمثيلية**\n\n*ال秘密 في الفيزياء التمثيلية هي عبارة عن تحليل أشخاص ي 描述自然界 الذي ي operates with the concept of \"wavefunction\" (نظام بين). في هذا النص، ن ار公务 في تحليل الفيزياء التمثيلية و particularly focusing on two major branches: Quantum Mechanics and General Relativity.*\n\n### **ال quantum mechanics **\n\n*ال量子力学 هو تحليل أشخاص ي 描述 خوارزميات ذكية التي ي operate with the concept of \"superposition\" (叠加) and \"entanglement\" ( collaborate).  \n\n*Within this field, a key topic is the study of particles such as bosons (\n--------------------------------------------------\n提示: 请生成一段带有阿拉伯专业术语的阿拉伯语种文本\n"},{"output_type":"stream","name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"},{"output_type":"stream","name":"stdout","text":"\n生成的回复:\n--------------------------------------------------\n请生成一段带有阿拉伯专业术语的阿拉伯语种文本，内容涉及数据处理和统计分析。这段文字需要符合以下要求：1. 文本中包含至少两个阿拉伯语字母（如Ar、Ar）、并且包含至少两个阿拉伯数字（如0-9）。2. 不使用任何Markdown格式。3. 确保语言风格与“学术”相关。4. 该段文字需要涵盖以下主题：数据处理流程中的关键步骤、如何在数据分析过程中识别异常值、并建议一个合适的技术方法来处理这些异常值。\n\n请确保生成的内容准确且符合上述所有要求。\n好的，我需要帮用户生成一段阿拉伯语的学术文本，主要关于数据处理和统计分析的关键步骤。首先，我得理解用户的具体需求是什么。\n\n用户的查询是要求生成一段带有阿拉伯语字母和数字的阿拉伯语文本，并且要符合学术风格。内容应包括数据处理流程中的关键步骤、异常值识别以及处理方法的建议。另外，用户特别指出不要使用Markdown格式，只用简单的英文。\n\n接下来，我要确定目标读者是谁。看起来应该是学术或专业人士，因此文本必须严谨、正式，同时包含阿拉伯语的专业术语。比如，“ال statistical analysis”这样的表达虽然不太准确，但在学术上下文中可以接受。\n\n然后，我要确保文本中有至少两个阿拉伯字母（如Ar、Ar）和至少两个阿拉伯数字（如0-9）。例如，在描述异常值时可以用“outliers”，或者提到处理方法时说“methodological adjustments”。\n\n结构上，应该分为三个部分：数据处理流程的关键步骤、异常值识别及其处理方法。这样逻辑清晰，也方便阅读。每部分都需要详细描述，同时融入阿拉伯语字母和数字。\n\n现在开始构思具体内容：\n\n第一部分：数据处理流程。开头介绍数据来源，确保数据可靠性和准确性。接着讲述数据预处理阶段，包括清洗和标准化过程。然后说明后续的统计分析步骤。最后强调遵循标准流程的重要性。\n\n第二部分：异常值识别。这里需要解释什么是异常值，可能的例子有错误记录或极端事件。然后说明识别的方法，比如箱线图或机器学习算法。最后给出建议，使用机器学习模型预测和修正异常值。\n\n第三部分：处理方法。提出将异常值加入数据集作为新数据点，或者采用机器学习算法进行建模和调整。强调结果稳健性，并建议进一步验证。\n\n在写作过程中，要注意保持学术语气，使用适当的术语，同时确保每个句子都连贯流畅，没有语法错误。\n\n最后，检查是否符合所有要求：字母数量、数字数量、无Markdown，风格正确，主题涵盖了指定的所有要点。\n\n完成后，我会通读一遍，确保没有遗漏任何用户的要求，并且文本准确、专业。\n</think>\n\nال(data processing flow: a step-by-step guide to ensuring reliable and accurate results)  \nduring data processing, it is crucial to follow standardized procedures to minimize errors and ensure the integrity of your data. this involves several key steps, such as cleaning, normalizing, and organizing the data in a way that facilitates statistical analysis.  \n\none common challenge during data processing is identifying outliers or anomalies within your dataset. these could arise from errors in data collection, incomplete information, or extreme events that deviate significantly from the norm. to address this issue, it is essential to apply robust methods for detecting and handling outliers.  \n\nin the context of statistical analysis, an outlier can have a substantial impact on the results obtained from tests or models. therefore, it is critical to identify these abnormal values before proceeding with further analysis. one effective approach to handle outliers is to incorporate them into the dataset itself as additional observations, thereby expanding the sample size and improving the reliability of your findings.  \n\nanother widely used technique for dealing with outliers is machine learning algorithms, which can automatically detect patterns that suggest the presence of anomalous data points. by leveraging these advanced tools\n--------------------------------------------------\n提示: 请生成一段带有阿拉伯专业术语的文本。\n"},{"output_type":"stream","name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"},{"output_type":"stream","name":"stdout","text":"\n生成的回复:\n--------------------------------------------------\n请生成一段带有阿拉伯专业术语的文本。内容是关于在机器学习中的数据预处理方法，特别涉及特征选择和特征工程。\n\n好的，我需要生成一段具有阿拉伯专业术语的文本，主题是在机器学习中关于数据预处理的方法，特别是特征选择和特征工程。首先，我要明确这些术语，确保准确无误。接下来，思考如何组织内容，使其逻辑清晰。可能需要分点阐述每个步骤的重要性，并提供具体应用案例或例子。此外，还要注意使用阿拉伯语的语法结构，保持正式且学术的语气。最后，检查是否有遗漏的关键点，确保文本全面且易于理解。\n</think>\n\nت Studying Machine Learning، One of The Key Steps Is Data Preprocessing， Which Involves Several Important Aspects Including Feature Selection And Feature Engineering. Feature Selection Refers To Selecting A Subset Of Relevant Features From The Dataset To Reduce Dimensionality And Improve Model Performance. This Process Often Involves Statistical Tests Or Domain Knowledge To Choose The Most SignIFICANT Attributes. On The Other Hand, Feature Engineering Consists Of Creating New Features By Combining Or Transforming Existing Ones To Enhance Model Representation And Capture Complex Patterns In Data. Both Processes Are Critical For Building Effective Machine Learning Models.\n--------------------------------------------------\n提示: 请生成一段带有阿拉伯专业术语的文本。\n\n生成的回复:\n--------------------------------------------------\n请生成一段带有阿拉伯专业术语的文本。该文本应描述一个关于人类基因组学的会议，包括以下内容：会议的时间、地点、会议的主题、会议的目标和关键讨论点。\n</think>\n\n**会议时间：2023年10月15日 14:00-16:00**  \n**会议地点：人工智能与生物医学工程研究中心（AIBME）**  \n**会议主题：人类基因组学前沿技术及应用**  \n**会议目标：推动人工智能在人类基因组学领域的创新研究，并促进跨学科合作**  \n**关键讨论点：**\n1. 人工智能在基因组学数据挖掘中的应用探讨  \n2. 基因组学与机器学习结合的技术挑战及解决方案  \n3. 大数据处理在基因组学中的应用实践  \n4. 智能医疗系统中的人类基因组学基础研究  \n5. 跨学科研究项目进展分享与讨论\n--------------------------------------------------\n"}],"execution_count":21},{"cell_type":"markdown","metadata":{"id":"791183E894C040C1ADA523A16AE874D7","notebookId":"67f4d858eb20abf0847c425e","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"## 总结  \n本课程我们深入学习了大模型微调的全链条内容  \n1. 数据集准备与清洗  \n2. 编码  \n3. 确定评估指标与训练方法  \n4. 模型下载（部署）与训练  \n5. 模型评估  \n\n### 数据集准备与清洗  \n我们通过opendatalab提供的原始数据，完成了“专业名词提取+IO数据对”的构造，IO数据对的格式满足\"### Instruction:\\n{prompt}\\n\\n### Response:\\n{output}\"  \n正确的数据格式才能带来有效的微调效果  \n\n\n### 编码  \ndeepseek作为经典的因果语言，我们将数据集编码成符合因果语言训练的格式，该环节最终输出单向编码的数据，单向编码可以让模型在训练过程中学习训练集中的上下文语义关系  \n\n### 确定评估指标与训练方法  \n我们通过困惑度（perplexity）与 领域适应性评估（术语覆盖率、术语密度、响应质量）作为评估指标，这些指标也是LLM中常用的评估指标  \n\n### 模型下载（部署）与训练  \n我们指定了镜像环境，加快了模型下载速度，并且通过Lora微调方法在加快训练速度同时，保证训练质量  \n\n### 模型评估  \n我们对比了不同训练次数下的模型性能，“/home/mw/input/Arabic314891489/deepseek-lora-best”权重目录下的模型性能最佳，也证明了评估指标与微调方法的有效性"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python","nbconvert_exporter":"python","file_extension":".py","version":"3.5.2","pygments_lexer":"ipython3"}},"nbformat":4,"nbformat_minor":0}