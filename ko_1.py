#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Gradio chat-demo for Qwen2-VL on MindSpore
-----------------------------------------
pip install "gradio==4.40.0" mindspore-cpu mindnlp pillow
python app.py
"""

import os
import base64
import time
import gradio as gr
import mindspore
from mindspore import context

from mindnlp.transformers import (
    Qwen2VLForConditionalGeneration,
    AutoProcessor,
)
from qwen_vl_utils import process_vision_info

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ MindSpore ç¯å¢ƒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
context.set_context(enable_debug_runtime=True)
context.set_context(mode=context.GRAPH_MODE, max_call_depth=2000)
print("Device target:", mindspore.get_context("device_target"))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ æ¨¡å‹ä¸å¤„ç†å™¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MODEL_PATH = "/home/HwHiAiUser/.cache/modelscope/hub/models/Qwen/Qwen2-VL-2B-Instruct"
FP16 = mindspore.float16

print("Loading model ... (é¦–æ¬¡å¯èƒ½è¾ƒæ…¢)")
model = Qwen2VLForConditionalGeneration.from_pretrained(
    MODEL_PATH,
    ms_dtype=FP16,
    trust_remote_code=True,
)

min_pixels = 256 * 28 * 28
max_pixels = 512 * 28 * 28
processor = AutoProcessor.from_pretrained(
    MODEL_PATH,
    min_pixels=min_pixels,
    max_pixels=max_pixels,
    ms_dtype=FP16,
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ å·¥å…·å‡½æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _pil_to_data_uri(pil_img):
    """å°† PIL.Image è½¬ä¸º data-URIï¼Œæ–¹ä¾¿åœ¨ browser ç«¯å›æ˜¾ã€‚"""
    import io
    buf = io.BytesIO()
    pil_img.save(buf, format="PNG")
    b64 = base64.b64encode(buf.getvalue()).decode()
    return f"data:image/png;base64,{b64}"

def build_messages(history):
    """
    å°†å†…éƒ¨ history( list[dict] ) è½¬ä¸º Qwen2-VL æœŸæœ›çš„ messages æ ¼å¼ã€‚
    history ç»“æ„è§ chat() ä¸‹æ–¹è¯´æ˜ã€‚
    """
    messages = []
    for turn in history:
        user_obj = {"role": "user", "content": []}

        if turn["user_img"] is not None:
            user_obj["content"].append(
                {"type": "image", "image": turn["user_img"]}
            )
        if turn["user_text"].strip():
            user_obj["content"].append(
                {"type": "text", "text": turn["user_text"]}
            )

        messages.append(user_obj)
        messages.append(
            {
                "role": "assistant",
                "content": turn["assistant"],
            }
        )
    return messages

def qwen2vl_generate(history):
    """
    1. æ ¹æ® history é‡æ–°æ‹¼è£…å®Œæ•´ messages
    2. è°ƒç”¨æ¨¡å‹ç”Ÿæˆ
    3. è¿”å› assistant æ–‡æœ¬
    """
    messages = build_messages(history)

    # Qwen2-VL ä¸“ç”¨æ¨¡æ¿
    prompt = processor.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )
    image_inputs, _ = process_vision_info(messages)

    inputs = processor(
        text=[prompt],
        images=image_inputs,
        padding=True,
        return_tensors="ms",
    )

    # MindSpore int64â†’int32
    for k in ("input_ids", "attention_mask", "position_ids"):
        if k in inputs and inputs[k].dtype == mindspore.int64:
            inputs[k] = inputs[k].astype(mindspore.int32)

    generated = model.generate(**inputs, max_new_tokens=256)

    # åªå–æ–°å¢ token
    trimmed = [
        out[len(src) :] for src, out in zip(inputs.input_ids, generated)
    ]
    text = processor.batch_decode(
        trimmed,
        skip_special_tokens=True,
        clean_up_tokenization_spaces=False,
    )[0]
    return text.strip()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Gradio å›è°ƒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def chat(user_text, user_image, state):
    """
    Gradio handler
    Parameters
    ----------
    user_text : str
    user_image : PIL.Image or None
    state : list[dict], gr.State
        [{
            "user_text": "...",
            "user_img": "/tmp/xxx.png" or None,
            "assistant": "..."
        }, ...]
    """
    state = state or []

    # ä¿å­˜ä¸Šä¼ å›¾ç‰‡åˆ°ä¸´æ—¶æ–‡ä»¶ï¼ˆè·¯å¾„ä¼ ç»™æ¨¡å‹ï¼‰
    img_path = None
    if user_image is not None:
        tmp_dir = "tmp_uploads"
        os.makedirs(tmp_dir, exist_ok=True)
        img_path = os.path.join(
            tmp_dir, f"{int(time.time()*1000)}.png"
        )
        user_image.save(img_path)

    # æ–° turnï¼ˆassistant æš‚ç½®ç©ºï¼Œå…ˆç”¨äºæ˜¾ç¤º userï¼‰
    state.append(
        {
            "user_text": user_text or "",
            "user_img": img_path,
            "assistant": "",
        }
    )

    # è®© chatbot å…ˆæ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
    chatbot_messages = []
    for t in state:
        user_display = ""
        if t["user_img"] is not None:
            user_display += f'<img src="{_pil_to_data_uri(gr.processing_utils.load_image(t["user_img"]))}" width="128"/> '
        user_display += t["user_text"]
        chatbot_messages.append((user_display, t["assistant"]))

    yield chatbot_messages, state  # ç«‹å³åˆ·æ–° UI

    # è°ƒç”¨æ¨¡å‹
    assistant_reply = qwen2vl_generate(state)
    state[-1]["assistant"] = assistant_reply

    # æ›´æ–° UI
    chatbot_messages[-1] = (chatbot_messages[-1][0], assistant_reply)
    yield chatbot_messages, state

def clear_fn():
    return [], []

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Gradio UI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with gr.Blocks(title="Qwen2-VL Demo ğŸš€") as demo:
    gr.Markdown(
        "<h1 style='text-align:center;'>Qwen2-VL ğŸ–¼ï¸ğŸ—£ï¸ MindSpore Demo</h1>"
    )
    chatbot = gr.Chatbot(
        [], elem_id="chatbot", height=480, bubble_full_width=False
    )
    with gr.Row():
        txt = gr.Textbox(
            lines=2,
            placeholder="è¯·è¾“å…¥æ–‡å­—â€¦",
            scale=4,
            container=False,
        )
        img = gr.Image(
            type="pil",
            label="ä¸Šä¼ å›¾ç‰‡",
            scale=1,
            height=120,
            tool=None,
        )
    submit = gr.Button("å‘é€ ğŸš€")
    clear = gr.Button("æ¸…ç©º ğŸ’«")
    state = gr.State([])

    submit.click(
        chat,
        inputs=[txt, img, state],
        outputs=[chatbot, state],
        show_progress="minimal",
    )
    clear.click(clear_fn, None, [chatbot, state])

demo.launch(server_name="0.0.0.0", server_port=7860, share=False)